{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "df_sup = pd.read_excel('df_surprise_labeled.xlsx')\n",
    "print(len(df_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    331\n",
      "1    269\n",
      "Name: Joy, dtype: int64\n",
      "0    457\n",
      "1    143\n",
      "Name: Sadness, dtype: int64\n",
      "0    541\n",
      "1     59\n",
      "Name: Anger, dtype: int64\n",
      "0    585\n",
      "1     15\n",
      "Name: Disgust, dtype: int64\n",
      "0    551\n",
      "1     49\n",
      "Name: Admiration, dtype: int64\n",
      "1    567\n",
      "0     33\n",
      "Name: Surprise, dtype: int64\n",
      "0    374\n",
      "1    226\n",
      "Name: Interest, dtype: int64\n",
      "0    577\n",
      "1     23\n",
      "Name: Fear, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for cl in df_sup.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    df_sup[cl].loc[(df_sup[cl]>0)]=1\n",
    "    print(df_sup[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    331\n",
      "1    269\n",
      "Name: Joy, dtype: int64\n",
      "0    457\n",
      "1    143\n",
      "Name: Sadness, dtype: int64\n",
      "0    541\n",
      "1     59\n",
      "Name: Anger, dtype: int64\n",
      "0    585\n",
      "1     15\n",
      "Name: Disgust, dtype: int64\n",
      "0    551\n",
      "1     49\n",
      "Name: Admiration, dtype: int64\n",
      "1    567\n",
      "0     33\n",
      "Name: Surprise, dtype: int64\n",
      "0    374\n",
      "1    226\n",
      "Name: Interest, dtype: int64\n",
      "0    577\n",
      "1     23\n",
      "Name: Fear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df_sup.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    print(df_sup[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7101\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand_7k.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7701\n"
     ]
    }
   ],
   "source": [
    "df = df.append(df_sup)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7701\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(7701).reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5794\n",
      "1    1907\n",
      "Name: Joy, dtype: int64\n",
      "0    4026\n",
      "1    3675\n",
      "Name: Sadness, dtype: int64\n",
      "0    6857\n",
      "1     844\n",
      "Name: Anger, dtype: int64\n",
      "0    6381\n",
      "1    1320\n",
      "Name: Disgust, dtype: int64\n",
      "0    7142\n",
      "1     559\n",
      "Name: Admiration, dtype: int64\n",
      "0    6900\n",
      "1     801\n",
      "Name: Surprise, dtype: int64\n",
      "0    6048\n",
      "1    1653\n",
      "Name: Interest, dtype: int64\n",
      "0    7089\n",
      "1     612\n",
      "Name: Fear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    print(df[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12590</th>\n",
       "      <td>faker</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4393</th>\n",
       "      <td>brutality</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19380</th>\n",
       "      <td>killjoy</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20880</th>\n",
       "      <td>lyricist</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#joy</td>\n",
       "      <td>0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21930</th>\n",
       "      <td>methanol</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34702</th>\n",
       "      <td>swab</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35546</th>\n",
       "      <td>thirteen-year-old</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>afrocentrist</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>aspd</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24198</th>\n",
       "      <td>objectivity</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Word    Primary    Secondary Polarity\n",
       "12590              faker   #sadness     #disgust    -0.82\n",
       "4393           brutality     #anger     #disgust    -0.96\n",
       "19380            killjoy   #sadness     #disgust    -0.75\n",
       "20880           lyricist       #joy         #joy    0.937\n",
       "21930           methanol   #sadness       #anger    -0.72\n",
       "34702               swab       #joy  #admiration    0.756\n",
       "35546  thirteen-year-old     #anger     #disgust    -0.84\n",
       "703         afrocentrist   #sadness     #disgust    -0.89\n",
       "2122                aspd  #surprise  #admiration    0.947\n",
       "24198        objectivity       #joy  #admiration    0.899"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15235\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Text'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4026\n",
       "1    3675\n",
       "Name: Sadness, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sadness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1907\n",
       "0    1907\n",
       "Name: Joy, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joy = df[df['Joy']==1]\n",
    "tmp = df[df['Joy']==0].sample(len(df_joy))\n",
    "df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "df_joy['Joy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "3814\n",
      "0.7085953878406709\n",
      "Sadness\n",
      "7350\n",
      "0.6653971708378672\n",
      "Anger\n",
      "1688\n",
      "0.6398104265402843\n",
      "Disgust\n",
      "2640\n",
      "0.5848484848484848\n",
      "Admiration\n",
      "1118\n",
      "0.6178571428571429\n",
      "Surprise\n",
      "1602\n",
      "0.7780548628428927\n",
      "Interest\n",
      "3306\n",
      "0.6614268440145102\n",
      "Fear\n",
      "1224\n",
      "0.6993464052287581\n"
     ]
    }
   ],
   "source": [
    "tag = []\n",
    "total_len = []\n",
    "train_cnt = []\n",
    "test_cnt = []\n",
    "accu = []\n",
    "conf_mat = []\n",
    "for cl in df.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    df_joy = df[df[cl]==1]\n",
    "    print(cl)\n",
    "    tag.append(cl)\n",
    "    tmp = df[df[cl]==0].sample(len(df_joy))\n",
    "    df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "    df_joy = df_joy.sample(len(df_joy)).reset_index(drop=True)\n",
    "    print(len(df_joy))\n",
    "    total_len.append(len(df_joy))\n",
    "    X = vectorizer.transform(df_joy['Text'])\n",
    "    Y = df_joy[cl]\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.25,random_state = 42)\n",
    "    train_cnt.append(len(y_train))\n",
    "    test_cnt.append(len(y_test))\n",
    "    rdforest = RandomForestClassifier(n_estimators=300)\n",
    "    rdforest.fit(x_train,y_train)\n",
    "    y_pred = rdforest.predict(x_test)\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    accu.append(accuracy_score(y_test,y_pred))\n",
    "    conf_mat.append(confusion_matrix(y_test,y_pred))\n",
    "    #classification_report.append(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Emotion  Total  Train  Test  Accuracy\n",
      "0         Joy   3814   2860   954  0.708595\n",
      "1     Sadness   7350   5512  1838  0.665397\n",
      "2       Anger   1688   1266   422  0.639810\n",
      "3     Disgust   2640   1980   660  0.584848\n",
      "4  Admiration   1118    838   280  0.617857\n",
      "5    Surprise   1602   1201   401  0.778055\n",
      "6    Interest   3306   2479   827  0.661427\n",
      "7        Fear   1224    918   306  0.699346\n"
     ]
    }
   ],
   "source": [
    "df_without_analysis = pd.DataFrame(list(zip(tag,total_len,train_cnt,test_cnt,accu)),\n",
    "                                   columns = ['Emotion','Total','Train','Test','Accuracy'])\n",
    "print(df_without_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7701\n"
     ]
    }
   ],
   "source": [
    "df_sum = df.drop('Text',axis=1)\n",
    "print(len(df_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4056\n",
      "2    2764\n",
      "3     498\n",
      "0     312\n",
      "4      62\n",
      "5       9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_sum_cnt = df_sum.sum(axis=1)\n",
    "print(df_sum_cnt.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[\"#disgust\"] = \"#interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_algo(sen):\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "        if(t in neg):\n",
    "            nflag=True\n",
    "        if(t[0] in punct):\n",
    "            nflag=False\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i fear life '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_algo(\"I feared for my life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7701\n",
      "7701\n"
     ]
    }
   ],
   "source": [
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6763\n",
      "1     938\n",
      "dtype: int64\n",
      "0    5533\n",
      "1    2168\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lol i thought be tucked up bed thanks info have think get back n ', 'need new job please help ', 'pep project be kick butt ', 'look job eb declare bankruptcy today ', 'up twitter world i need help boy problem act so sweet friend s jerk i need boy help ', 'aaaah i need stop sleep late i dread work be i m sleepies ', 'drink hot coffee shirt be prepared burn chest ', 'yeah do i have say i felt so bad um twitter ate rainbow paddle pop xx ', 'commercial shoot be cancel next week too gray nice shot building dress nice nothing jk cool ', 'flood message phone phone be still ill not now please be howl pain ']\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "print(analysed[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@daisyclover1938 lol... I thought you were tuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>lol i thought be tucked up bed thanks info hav...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>needs a new job. please help</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>need new job please help</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this pep8 project is kicking our butts.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pep project be kick butt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking for jobs, EB declared bankruptcy today</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>look job eb declare bankruptcy today</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What up twitter world .I need help boy problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>up twitter world i need help boy problem act s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aaaah! I need to stop sleeping late! I dread w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aaaah i need stop sleep late i dread work be i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you drink hot coffee with your shirt off......</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>drink hot coffee shirt be prepared burn chest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@charlii1 yeah it did and I had to say no to a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah do i have say i felt so bad um twitter at...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>commercial shoot was cancelled until next week...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>commercial shoot be cancel next week too gray ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Another flood of messages in my phone.  my pho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>flood message phone phone be still ill not now...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0  @daisyclover1938 lol... I thought you were tuc...    0        0      0   \n",
       "1                      needs a new job. please help     1        0      0   \n",
       "2           this pep8 project is kicking our butts.     0        0      1   \n",
       "3    looking for jobs, EB declared bankruptcy today     0        1      0   \n",
       "4  What up twitter world .I need help boy problem...    1        0      0   \n",
       "5  Aaaah! I need to stop sleeping late! I dread w...    0        1      0   \n",
       "6  If you drink hot coffee with your shirt off......    0        0      0   \n",
       "7  @charlii1 yeah it did and I had to say no to a...    0        1      0   \n",
       "8  commercial shoot was cancelled until next week...    1        0      1   \n",
       "9  Another flood of messages in my phone.  my pho...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         1     0   \n",
       "1        0           0         0         0     0   \n",
       "2        1           0         0         0     0   \n",
       "3        0           0         0         1     0   \n",
       "4        0           1         1         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        0           0         0         0     1   \n",
       "7        0           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        0           0         0         0     1   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0  lol i thought be tucked up bed thanks info hav...      1       1  \n",
       "1                          need new job please help       0       0  \n",
       "2                          pep project be kick butt       0       0  \n",
       "3              look job eb declare bankruptcy today       0       0  \n",
       "4  up twitter world i need help boy problem act s...      0       1  \n",
       "5  aaaah i need stop sleep late i dread work be i...      0       1  \n",
       "6     drink hot coffee shirt be prepared burn chest       0       0  \n",
       "7  yeah do i have say i felt so bad um twitter at...      0       1  \n",
       "8  commercial shoot be cancel next week too gray ...      0       1  \n",
       "9  flood message phone phone be still ill not now...      1       0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9606\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1907, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = 'Joy'\n",
    "df_joy = df[df[cl]==1]\n",
    "X = df_joy[['Analysed','qmark','exmark']]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814\n",
      "(3350, 9608)\n",
      "(954, 9608)\n",
      "0.7064989517819706\n"
     ]
    }
   ],
   "source": [
    "cl = 'Joy'\n",
    "df_joy = df[df[cl]==1]\n",
    "#print(cl)\n",
    "#tag.append(cl)\n",
    "tmp = df[df[cl]==0].sample(len(df_joy))\n",
    "df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "df_joy = df_joy.sample(len(df_joy)).reset_index(drop=True)\n",
    "print(len(df_joy))\n",
    "total_len.append(len(df_joy))\n",
    "#X = vectorizer.transform(df_joy['Analysed'])\n",
    "X = df_joy[['Analysed','qmark','exmark']]\n",
    "Y = df_joy[cl].tolist()\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.25,random_state = 42)\n",
    "x_train_analysed = x_train['Analysed'].tolist()\n",
    "x_train_qmark = x_train['qmark'].tolist()\n",
    "x_train_exmark = x_train['exmark'].tolist()\n",
    "x_test_analysed = x_test['Analysed'].tolist()\n",
    "x_test_qmark = x_test['qmark'].tolist()\n",
    "x_test_exmark = x_test['exmark'].tolist()\n",
    "pre = {}\n",
    "for sen in x_train_analysed:\n",
    "    tok = word_tokenize(sen)\n",
    "    for t in tok:\n",
    "        pre[t]=1\n",
    "for sen in x_test_analysed:\n",
    "    tok = word_tokenize(sen)\n",
    "    for t in tok:\n",
    "        if(t in pre):\n",
    "            continue\n",
    "        else:\n",
    "            if(t in senticnet):\n",
    "                x_train_analysed.append(t)\n",
    "                x_train_qmark.append(0)\n",
    "                x_train_exmark.append(0)\n",
    "                y_train.append(get_senticnet(t,cl))\n",
    "x_train_analysed = vectorizer.transform(x_train_analysed)\n",
    "x_test_analysed = vectorizer.transform(x_test_analysed)\n",
    "tmp = sparse.hstack((x_train_analysed,np.array(x_train_qmark)[:,None]))\n",
    "x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "print(x_train.shape)\n",
    "tmp = sparse.hstack((x_test_analysed,np.array(x_test_qmark)[:,None]))\n",
    "x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "print(x_test.shape)\n",
    "rdforest = RandomForestClassifier(n_estimators=300)\n",
    "rdforest.fit(x_train,y_train)\n",
    "y_pred = rdforest.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "3814\n",
      "0.7012578616352201\n",
      "Sadness\n",
      "7350\n",
      "0.6632208922742111\n",
      "Anger\n",
      "1688\n",
      "0.6208530805687204\n",
      "Disgust\n",
      "2640\n",
      "0.6348484848484849\n",
      "Admiration\n",
      "1118\n",
      "0.575\n",
      "Surprise\n",
      "1602\n",
      "0.7855361596009975\n",
      "Interest\n",
      "3306\n",
      "0.6928657799274486\n",
      "Fear\n",
      "1224\n",
      "0.7745098039215687\n",
      "qmark\n",
      "1876\n",
      "0.997867803837953\n",
      "exmark\n",
      "4336\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tag = []\n",
    "total_len = []\n",
    "train_cnt = []\n",
    "test_cnt = []\n",
    "accu = []\n",
    "conf_mat = []\n",
    "for cl in df.columns:\n",
    "    if(cl=='Text' or cl=='Analysed'):\n",
    "        continue\n",
    "    df_joy = df[df[cl]==1]\n",
    "    print(cl)\n",
    "    tag.append(cl)\n",
    "    tmp = df[df[cl]==0].sample(len(df_joy))\n",
    "    df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "    df_joy = df_joy.sample(len(df_joy)).reset_index(drop=True)\n",
    "    print(len(df_joy))\n",
    "    total_len.append(len(df_joy))\n",
    "    #X = vectorizer.transform(df_joy['Analysed'])\n",
    "    X = df_joy[['Analysed','qmark','exmark']]\n",
    "    Y = df_joy[cl].tolist()\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.25,random_state = 42)\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    y_train.append(get_senticnet(t,cl))\n",
    "    x_train_analysed = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    #print(x_train.shape)\n",
    "    tmp = sparse.hstack((x_test_analysed,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    #print(x_test.shape)\n",
    "    rdforest = RandomForestClassifier(n_estimators=300)\n",
    "    rdforest.fit(x_train,y_train)\n",
    "    y_pred = rdforest.predict(x_test)\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    accu.append(accuracy_score(y_test,y_pred))\n",
    "    #conf_mat.append(confusion_matrix(y_test,y_pred))\n",
    "    #classification_report.append(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Emotion, Total, Train, Test, Accuracy]\n",
      "Index: []\n",
      "      Emotion  Total  Train  Test  Accuracy\n",
      "0         Joy   3814   2860   954  0.708595\n",
      "1     Sadness   7350   5512  1838  0.665397\n",
      "2       Anger   1688   1266   422  0.639810\n",
      "3     Disgust   2640   1980   660  0.584848\n",
      "4  Admiration   1118    838   280  0.617857\n",
      "5    Surprise   1602   1201   401  0.778055\n",
      "6    Interest   3306   2479   827  0.661427\n",
      "7        Fear   1224    918   306  0.699346\n"
     ]
    }
   ],
   "source": [
    "df_with_analysis = pd.DataFrame(list(zip(tag,total_len,train_cnt,test_cnt,accu)),\n",
    "                                   columns = ['Emotion','Total','Train','Test','Accuracy'])\n",
    "print(df_with_analysis)\n",
    "print(df_without_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame()\n",
    "for cl in df.columns:\n",
    "    if(cl==\"Text\" or cl==\"Analysed\" or cl=='qmark' or cl=='exmark'):\n",
    "        continue\n",
    "    df_test = df_test.append(df[df[cl]==1].sample(100))\n",
    "    print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    560\n",
      "1    240\n",
      "Name: Joy, dtype: int64\n",
      "0    493\n",
      "1    307\n",
      "Name: Sadness, dtype: int64\n",
      "0    631\n",
      "1    169\n",
      "Name: Anger, dtype: int64\n",
      "0    628\n",
      "1    172\n",
      "Name: Disgust, dtype: int64\n",
      "0    667\n",
      "1    133\n",
      "Name: Admiration, dtype: int64\n",
      "0    637\n",
      "1    163\n",
      "Name: Surprise, dtype: int64\n",
      "0    586\n",
      "1    214\n",
      "Name: Interest, dtype: int64\n",
      "0    671\n",
      "1    129\n",
      "Name: Fear, dtype: int64\n",
      "0    690\n",
      "1    110\n",
      "Name: qmark, dtype: int64\n",
      "0    558\n",
      "1    242\n",
      "Name: exmark, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df_test.columns:\n",
    "    if(cl==\"Text\" or cl==\"Analysed\"):\n",
    "        continue\n",
    "    print(df_test[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>@Area224 I can relate...I had a guard because ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i relate i have guard i clench jaw still do i ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>@DarthSLR  Time to make lemonaide, Nik!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>time make lemonaide nik</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>hahaha math exam was shit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hahaha math exam be shit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>@GeemaPee Just been to asda, now at the fort w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>just be asda now fort wait hour photo everythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>@fyldelibraries i am looking forward to QUEST ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i be look forward quest seeker woohoo shame ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Joy  Sadness  Anger  \\\n",
       "3233  @Area224 I can relate...I had a guard because ...    1        0      0   \n",
       "2846            @DarthSLR  Time to make lemonaide, Nik!    1        0      0   \n",
       "2721                         hahaha math exam was shit     1        0      0   \n",
       "3812  @GeemaPee Just been to asda, now at the fort w...    1        0      0   \n",
       "5417  @fyldelibraries i am looking forward to QUEST ...    1        0      0   \n",
       "\n",
       "      Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "3233        0           0         0         0     0   \n",
       "2846        0           0         0         0     0   \n",
       "2721        0           0         0         0     0   \n",
       "3812        0           0         0         0     0   \n",
       "5417        0           0         1         0     0   \n",
       "\n",
       "                                               Analysed  qmark  exmark  \n",
       "3233  i relate i have guard i clench jaw still do i ...      0       1  \n",
       "2846                           time make lemonaide nik       0       1  \n",
       "2721                          hahaha math exam be shit       0       0  \n",
       "3812  just be asda now fort wait hour photo everythi...      0       1  \n",
       "5417  i be look forward quest seeker woohoo shame ca...      0       1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6919\n",
      "782\n"
     ]
    }
   ],
   "source": [
    "df = df[~df.index.isin(df_test.index)]\n",
    "df_test = df_test[~df_test.index.duplicated(keep='first')].reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)\n",
    "print(len(df))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test_analysed = vectorizer.transform(df_test['Analysed'])\n",
    "g_test_qmark = df_test['qmark'].tolist()\n",
    "g_test_exmark = df_test['exmark'].tolist()\n",
    "tmp = sparse.hstack((g_test_analysed,np.array(g_test_qmark)[:,None]))\n",
    "g_test = sparse.hstack((tmp,np.array(g_test_exmark)[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "3350\n",
      "added 360\n",
      "0.7149253731343284\n",
      "Sadness\n",
      "6746\n",
      "added 473\n",
      "0.6748148148148149\n",
      "Anger\n",
      "1366\n",
      "added 231\n",
      "0.6496350364963503\n",
      "Disgust\n",
      "2300\n",
      "added 325\n",
      "0.6282608695652174\n",
      "Admiration\n",
      "862\n",
      "added 198\n",
      "0.6242774566473989\n",
      "Surprise\n",
      "1290\n",
      "added 276\n",
      "0.7790697674418605\n",
      "Interest\n",
      "2888\n",
      "added 329\n",
      "0.6920415224913494\n",
      "Fear\n",
      "972\n",
      "added 222\n",
      "0.7282051282051282\n"
     ]
    }
   ],
   "source": [
    "tag = []\n",
    "total_len = []\n",
    "train_cnt = []\n",
    "test_cnt = []\n",
    "accu = []\n",
    "conf_mat = []\n",
    "predict_score_list = []\n",
    "for cl in df.columns:\n",
    "    if(cl=='Text' or cl=='Analysed' or cl=='qmark' or cl=='exmark'):\n",
    "        continue\n",
    "    df_joy = df[df[cl]==1]\n",
    "    print(cl)\n",
    "    tag.append(cl)\n",
    "    tmp = df[df[cl]==0].sample(len(df_joy))\n",
    "    df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "    df_joy = df_joy.sample(len(df_joy)).reset_index(drop=True)\n",
    "    print(len(df_joy))\n",
    "    total_len.append(len(df_joy))\n",
    "    #X = vectorizer.transform(df_joy['Analysed'])\n",
    "    X = df_joy[['Analysed','qmark','exmark']]\n",
    "    Y = df_joy[cl].tolist()\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    add_cnt = 0\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    add_cnt+=1\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    y_train.append(get_senticnet(t,cl))\n",
    "    x_train_analysed = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    #print(x_train.shape)\n",
    "    tmp = sparse.hstack((x_test_analysed,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    #print(x_test.shape)\n",
    "    print(\"added \"+str(add_cnt))\n",
    "    rdforest = RandomForestClassifier(n_estimators=300)\n",
    "    rdforest.fit(x_train,y_train)\n",
    "    y_pred = rdforest.predict(x_test)\n",
    "    gl_pred = rdforest.predict(g_test)\n",
    "    predict_score_list.append(gl_pred)\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    accu.append(accuracy_score(y_test,y_pred))\n",
    "    conf_mat.append(confusion_matrix(y_test,y_pred))\n",
    "    #classification_report.append(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "Sadness\n",
      "Anger\n",
      "Disgust\n",
      "Admiration\n",
      "Surprise\n",
      "Interest\n",
      "Fear\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for cl in df_test.columns:\n",
    "    if(cl==\"Text\" or cl==\"Analysed\" or cl=='qmark' or cl=='exmark'):\n",
    "        continue\n",
    "    print(cl)\n",
    "    score_list.append(df_test[cl].tolist())\n",
    "print(len(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Cardinality: 1.89769820971867\n",
      "Label Density: 0.23721227621483376\n",
      "Hamming Loss: 0.24952046035805628\n",
      "Exact Prediction: 0.09207161125319693\n",
      "At least one label predicted: 0.7135549872122762\n",
      "More than one label predicted: 0.4213381555153707\n"
     ]
    }
   ],
   "source": [
    "filter_corr = []\n",
    "exmatch = 0\n",
    "atleast1 = 0\n",
    "md1 = 0\n",
    "one_f = 0\n",
    "more_f = 0\n",
    "zero_f = 0\n",
    "sm = 0\n",
    "sdensity = 0\n",
    "hammval = 0\n",
    "test_len = len(predict_score_list[0])\n",
    "for j in range(test_len):\n",
    "    cnt=0\n",
    "    for i in range(8):\n",
    "        hammval+=(score_list[i][j] ^ predict_score_list[i][j])\n",
    "        if(score_list[i][j]==1):\n",
    "            cnt+=1\n",
    "            sm+=1\n",
    "    sdensity+=cnt/8\n",
    "    if(cnt==0):\n",
    "        zero_f+=1\n",
    "    if(cnt==1):\n",
    "        one_f+=1\n",
    "    if(cnt>1):\n",
    "        more_f+=1\n",
    "    for i in range(8):\n",
    "        mf = True\n",
    "        if(predict_score_list[i][j]!=score_list[i][j]):\n",
    "            mf=False\n",
    "            break\n",
    "    if(mf==True):\n",
    "        exmatch+=1\n",
    "        filter_corr.append(j)\n",
    "    for i in range(8):\n",
    "        if(predict_score_list[i][j]==score_list[i][j] and score_list[i][j]==1):\n",
    "            atleast1+=1\n",
    "            break\n",
    "    mf = False\n",
    "    for i in range(8):\n",
    "        if(predict_score_list[i][j]==score_list[i][j] and score_list[i][j]==1):\n",
    "            if(mf==True):\n",
    "                md1+=1\n",
    "                filter_corr.append(j)\n",
    "                break\n",
    "            mf=True\n",
    "print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "print(\"More than one label predicted: \"+str(md1/more_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7872954764196343\n",
      "1.5195469977178089\n",
      "2.351507459002817\n",
      "3.1870337747922908\n",
      "4.042151885028511\n",
      "4.892846329472956\n",
      "5.701202369436626\n",
      "6.622195597427597\n",
      "0.8116093880972338 0.8540242557883131 0.8322767809175889\n",
      "0.8076941923011481 0.8509647151782322 0.8277744496784496\n"
     ]
    }
   ],
   "source": [
    "tp_sum = 0\n",
    "fp_sum = 0\n",
    "fn_sum = 0\n",
    "macro_preci = 0\n",
    "macro_recall = 0\n",
    "macro_f1 = 0\n",
    "for i in range(len(score_list)):\n",
    "    tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "    tp_sum+=tmp[0][0]\n",
    "    fp_sum+=tmp[0][1]\n",
    "    fn_sum+=tmp[1][0]\n",
    "    macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "    macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "    macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "    macro_preci+=macro_preci_tmp\n",
    "    macro_recall+=macro_recall_tmp\n",
    "    print(macro_f1)\n",
    "micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "macro_preci/=8\n",
    "macro_recall/=8\n",
    "macro_f1/=8\n",
    "print(micro_preci,micro_recall,micro_f1)\n",
    "print(macro_preci,macro_recall,macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504795396419437\n"
     ]
    }
   ],
   "source": [
    "tmp = 0\n",
    "for i in range(len(score_list)):\n",
    "    tmp += accuracy_score(score_list[i],predict_score_list[i])\n",
    "print(tmp/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "3350\n",
      "added 355\n",
      "0.6985074626865672\n",
      "Sadness\n",
      "6746\n",
      "added 500\n",
      "0.6585185185185185\n",
      "Anger\n",
      "1366\n",
      "added 294\n",
      "0.5656934306569343\n",
      "Disgust\n",
      "2300\n",
      "added 356\n",
      "0.6108695652173913\n",
      "Admiration\n",
      "862\n",
      "added 193\n",
      "0.5375722543352601\n",
      "Surprise\n",
      "1290\n",
      "added 244\n",
      "0.6744186046511628\n",
      "Interest\n",
      "2888\n",
      "added 349\n",
      "0.7076124567474048\n",
      "Fear\n",
      "972\n",
      "added 198\n",
      "0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "tag = []\n",
    "total_len = []\n",
    "train_cnt = []\n",
    "test_cnt = []\n",
    "accu = []\n",
    "conf_mat = []\n",
    "predict_score_list = []\n",
    "for cl in df.columns:\n",
    "    if(cl=='Text' or cl=='Analysed' or cl=='qmark' or cl=='exmark'):\n",
    "        continue\n",
    "    df_joy = df[df[cl]==1]\n",
    "    print(cl)\n",
    "    tag.append(cl)\n",
    "    tmp = df[df[cl]==0].sample(len(df_joy))\n",
    "    df_joy = df_joy.append(tmp).reset_index(drop=True)\n",
    "    df_joy = df_joy.sample(len(df_joy)).reset_index(drop=True)\n",
    "    print(len(df_joy))\n",
    "    total_len.append(len(df_joy))\n",
    "    #X = vectorizer.transform(df_joy['Analysed'])\n",
    "    X = df_joy[['Analysed','qmark','exmark']]\n",
    "    Y = df_joy[cl].tolist()\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    add_cnt = 0\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    add_cnt+=1\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    y_train.append(get_senticnet(t,cl))\n",
    "    x_train_analysed = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    #print(x_train.shape)\n",
    "    tmp = sparse.hstack((x_test_analysed,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    #print(x_test.shape)\n",
    "    print(\"added \"+str(add_cnt))\n",
    "    rdforest = SVC()\n",
    "    rdforest.fit(x_train,y_train)\n",
    "    y_pred = rdforest.predict(x_test)\n",
    "    gl_pred = rdforest.predict(g_test)\n",
    "    predict_score_list.append(gl_pred)\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    accu.append(accuracy_score(y_test,y_pred))\n",
    "    conf_mat.append(confusion_matrix(y_test,y_pred))\n",
    "    #classification_report.append(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n",
      "Sadness\n",
      "Anger\n",
      "Disgust\n",
      "Admiration\n",
      "Surprise\n",
      "Interest\n",
      "Fear\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for cl in df_test.columns:\n",
    "    if(cl==\"Text\" or cl==\"Analysed\" or cl=='qmark' or cl=='exmark'):\n",
    "        continue\n",
    "    print(cl)\n",
    "    score_list.append(df_test[cl].tolist())\n",
    "print(len(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Cardinality: 1.89769820971867\n",
      "Label Density: 0.23721227621483376\n",
      "Hamming Loss: 0.27925191815856776\n",
      "Exact Prediction: 0.07289002557544758\n",
      "At least one label predicted: 0.6675191815856778\n",
      "More than one label predicted: 0.35443037974683544\n"
     ]
    }
   ],
   "source": [
    "filter_corr = []\n",
    "exmatch = 0\n",
    "atleast1 = 0\n",
    "md1 = 0\n",
    "one_f = 0\n",
    "more_f = 0\n",
    "zero_f = 0\n",
    "sm = 0\n",
    "sdensity = 0\n",
    "hammval = 0\n",
    "test_len = len(predict_score_list[0])\n",
    "for j in range(test_len):\n",
    "    cnt=0\n",
    "    for i in range(8):\n",
    "        hammval+=(score_list[i][j] ^ predict_score_list[i][j])\n",
    "        if(score_list[i][j]==1):\n",
    "            cnt+=1\n",
    "            sm+=1\n",
    "    sdensity+=cnt/8\n",
    "    if(cnt==0):\n",
    "        zero_f+=1\n",
    "    if(cnt==1):\n",
    "        one_f+=1\n",
    "    if(cnt>1):\n",
    "        more_f+=1\n",
    "    for i in range(8):\n",
    "        mf = True\n",
    "        if(predict_score_list[i][j]!=score_list[i][j]):\n",
    "            mf=False\n",
    "            break\n",
    "    if(mf==True):\n",
    "        exmatch+=1\n",
    "        filter_corr.append(j)\n",
    "    for i in range(8):\n",
    "        if(predict_score_list[i][j]==score_list[i][j] and score_list[i][j]==1):\n",
    "            atleast1+=1\n",
    "            break\n",
    "    mf = False\n",
    "    for i in range(8):\n",
    "        if(predict_score_list[i][j]==score_list[i][j] and score_list[i][j]==1):\n",
    "            if(mf==True):\n",
    "                md1+=1\n",
    "                filter_corr.append(j)\n",
    "                break\n",
    "            mf=True\n",
    "print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "print(\"More than one label predicted: \"+str(md1/more_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568093385214008\n",
      "1.4915440732561356\n",
      "2.2362249243199654\n",
      "3.0489339544203\n",
      "3.8381866832196283\n",
      "4.708968576223744\n",
      "5.551777940772239\n",
      "6.459102305495707\n",
      "0.7904442581726739 0.8346979420225714 0.8119685717360887\n",
      "0.7888613246918792 0.8317392581943708 0.8073877881869633\n"
     ]
    }
   ],
   "source": [
    "tp_sum = 0\n",
    "fp_sum = 0\n",
    "fn_sum = 0\n",
    "macro_preci = 0\n",
    "macro_recall = 0\n",
    "macro_f1 = 0\n",
    "for i in range(len(score_list)):\n",
    "    tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "    tp_sum+=tmp[0][0]\n",
    "    fp_sum+=tmp[0][1]\n",
    "    fn_sum+=tmp[1][0]\n",
    "    macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "    macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "    macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "    macro_preci+=macro_preci_tmp\n",
    "    macro_recall+=macro_recall_tmp\n",
    "    print(macro_f1)\n",
    "micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "macro_preci/=8\n",
    "macro_recall/=8\n",
    "macro_f1/=8\n",
    "print(micro_preci,micro_recall,micro_f1)\n",
    "print(macro_preci,macro_recall,macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7207480818414321\n"
     ]
    }
   ],
   "source": [
    "tmp = 0\n",
    "for i in range(len(score_list)):\n",
    "    tmp += accuracy_score(score_list[i],predict_score_list[i])\n",
    "print(tmp/8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
