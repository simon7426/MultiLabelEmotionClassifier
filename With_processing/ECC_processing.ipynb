{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6095\n",
      "1    2406\n",
      "Name: Joy, dtype: int64\n",
      "0    4375\n",
      "1    4126\n",
      "Name: Sadness, dtype: int64\n",
      "0    7512\n",
      "1     989\n",
      "Name: Anger, dtype: int64\n",
      "0    7121\n",
      "1    1380\n",
      "Name: Disgust, dtype: int64\n",
      "0    7923\n",
      "1     578\n",
      "Name: Admiration, dtype: int64\n",
      "0    7877\n",
      "1     624\n",
      "Name: Surprise, dtype: int64\n",
      "0    6367\n",
      "1    2134\n",
      "Name: Interest, dtype: int64\n",
      "0    7827\n",
      "1     674\n",
      "Name: Fear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    print(df[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37324</th>\n",
       "      <td>unhygienic</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>binoculars</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24910</th>\n",
       "      <td>outrage</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>condescendingly</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13683</th>\n",
       "      <td>fossil</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11238</th>\n",
       "      <td>emotionally</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39381</th>\n",
       "      <td>withstand</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34432</th>\n",
       "      <td>sunscald</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>clon</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35981</th>\n",
       "      <td>tot</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Word      Primary    Secondary Polarity\n",
       "37324       unhygienic        #fear     #disgust    -0.07\n",
       "3449        binoculars       #anger     #disgust    -0.02\n",
       "24910          outrage       #anger     #disgust    -0.31\n",
       "6851   condescendingly    #surprise  #admiration    0.831\n",
       "13683           fossil  #admiration  #admiration    0.882\n",
       "11238      emotionally         #joy    #interest    0.945\n",
       "39381        withstand    #surprise  #admiration    0.357\n",
       "34432         sunscald        #fear     #disgust    -0.95\n",
       "6146              clon    #interest  #admiration    0.063\n",
       "35981              tot     #sadness       #anger    -0.74"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "8501\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "negatives = set(negatives)\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7425\n",
      "1    1076\n",
      "dtype: int64\n",
      "0    6043\n",
      "1    2458\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So much for sleeping in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so much sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College days are loooong days.. 3 more hours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>college day be loooong day more hour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@daihard I'm headed to Kentucky this time. Nev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i m head kentucky time never be be fun gqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hella tired.. where is gilbert for the usual b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hella tire be gilbert usual basketball talk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not as dry this morning as would have liked  l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not dry morning a have like lot moisture dune ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lil_laura_loo Really? I think we have some! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>laura loo really i think have i ve take pirite...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Do512_Kristin it's a good thing they give you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kristin s good thing give xanax something i ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PB&amp;amp;J, Owl City, and boredom.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pb amp j owl city boredom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So it's Saturday again &amp;amp; what do I do..? W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s saturday again amp do i do work again course</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trying to relax and watch Nascar, Difficult 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>try relax watch nascar difficult cause child d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0                          So much for sleeping in.     0        0      0   \n",
       "1      College days are loooong days.. 3 more hours     0        1      0   \n",
       "2  @daihard I'm headed to Kentucky this time. Nev...    0        0      0   \n",
       "3  hella tired.. where is gilbert for the usual b...    0        0      0   \n",
       "4  Not as dry this morning as would have liked  l...    0        1      0   \n",
       "5  @lil_laura_loo Really? I think we have some! I...    0        1      0   \n",
       "6  @Do512_Kristin it's a good thing they give you...    0        0      0   \n",
       "7                  PB&amp;J, Owl City, and boredom.     0        1      0   \n",
       "8  So it's Saturday again &amp; what do I do..? W...    0        1      0   \n",
       "9  trying to relax and watch Nascar, Difficult 'c...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         0     1   \n",
       "1        0           0         0         1     0   \n",
       "2        0           0         0         1     0   \n",
       "3        0           0         0         1     0   \n",
       "4        1           0         0         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        1           0         0         0     0   \n",
       "7        1           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        1           0         0         0     0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0                                     so much sleep       0       0  \n",
       "1              college day be loooong day more hour       0       0  \n",
       "2        i m head kentucky time never be be fun gqz       1       1  \n",
       "3       hella tire be gilbert usual basketball talk       1       1  \n",
       "4  not dry morning a have like lot moisture dune ...      0       1  \n",
       "5  laura loo really i think have i ve take pirite...      1       1  \n",
       "6  kristin s good thing give xanax something i ba...      0       0  \n",
       "7                         pb amp j owl city boredom       0       0  \n",
       "8    s saturday again amp do i do work again course       1       0  \n",
       "9  try relax watch nascar difficult cause child d...      0       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Analysed','qmark','exmark']]\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]\n",
    "Y = Y.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10952\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8470, 10954)\n",
      "(8470, 8)\n",
      "(2126, 10954)\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.25,random_state = 42)\n",
    "x_train_analysed = x_train['Analysed'].tolist()\n",
    "x_train_qmark = x_train['qmark'].tolist()\n",
    "x_train_exmark = x_train['exmark'].tolist()\n",
    "x_test_analysed = x_test['Analysed'].tolist()\n",
    "x_test_qmark = x_test['qmark'].tolist()\n",
    "x_test_exmark = x_test['exmark'].tolist()\n",
    "pre = {}\n",
    "for sen in x_train_analysed:\n",
    "    tok = word_tokenize(sen)\n",
    "    for t in tok:\n",
    "        pre[t]=1\n",
    "for sen in x_test_analysed:\n",
    "    tok = word_tokenize(sen)\n",
    "    for t in tok:\n",
    "        if(t in pre):\n",
    "            continue\n",
    "        else:\n",
    "            if(t in senticnet):\n",
    "                x_train_analysed.append(t)\n",
    "                x_train_qmark.append(0)\n",
    "                x_train_exmark.append(0)\n",
    "                tmp_list = []\n",
    "                for cl in col_names:\n",
    "                    tmp_list.append(get_senticnet(t,cl))\n",
    "                y_train.append(tmp_list)\n",
    "for word in negatives:\n",
    "    if(word in senticnet):\n",
    "        x_train_analysed.append(\"not \"+word)\n",
    "        x_train_qmark.append(0)\n",
    "        x_train_exmark.append(0)\n",
    "        tmp_list = []\n",
    "        for cl in col_names:\n",
    "            tmp_list.append(get_senticnet(word,cl))\n",
    "        tmp_list2 = []\n",
    "        for i in range(8):\n",
    "            tmp_list2.append(0)\n",
    "        for i in range(8):\n",
    "            if(tmp_list[i]==1):\n",
    "                tmp_list2[opposite[i]] = 1\n",
    "        y_train.append(tmp_list2)\n",
    "x_train_analysed = vectorizer.transform(x_train_analysed)\n",
    "x_test_analysed = vectorizer.transform(x_test_analysed)\n",
    "tmp = sparse.hstack((x_train_analysed,np.array(x_train_qmark)[:,None]))\n",
    "x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "tmp = sparse.hstack((x_test_analysed,np.array(x_test_qmark)[:,None]))\n",
    "x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Chain 1\n",
      "Training Chain 2\n",
      "Training Chain 3\n",
      "Training Chain 4\n",
      "Training Chain 5\n",
      "Training Chain 6\n",
      "Training Chain 7\n",
      "Training Chain 8\n",
      "Training Chain 9\n",
      "Training Chain 10\n"
     ]
    }
   ],
   "source": [
    "base = RandomForestClassifier()\n",
    "\n",
    "chains = [ClassifierChain(base,order = 'random',random_state=i) for i in range(10)]\n",
    "cnt=1\n",
    "y_pred_list = []\n",
    "for chain in chains:\n",
    "    print(\"Training Chain \"+ str(cnt))\n",
    "    cnt+=1\n",
    "    chain.fit(x_train,y_train)\n",
    "    y_pred = chain.predict(x_test)\n",
    "    y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.16239416745061147\n",
      "Exact Prediction: 0.2605832549388523\n",
      "0.8498789346246973\n",
      "1.487885773901688\n",
      "2.430207871279965\n",
      "3.3464032440306077\n",
      "4.308750432294667\n",
      "5.267339013524757\n",
      "6.128165266088484\n",
      "7.086189957446509\n",
      "Macro F-Score: 0.8857737446808136\n",
      "Micro F-Score: 0.9043628808864266\n",
      "Joy accuracy: 0.7666980244590781\n",
      "Sadness accuracy: 0.6514581373471308\n",
      "Anger accuracy: 0.8913452492944497\n",
      "Disgust accuracy: 0.8466603951081844\n",
      "Admiration accuracy: 0.9275634995296331\n",
      "Surprise accuracy: 0.9205079962370649\n",
      "Interest accuracy: 0.7765757290686736\n",
      "Fear accuracy: 0.9200376293508937\n",
      "Average Accuracy: 0.8376058325493885\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "threshold = 5\n",
    "for i in range(len(y_pred_list[0])):\n",
    "    tmp = []\n",
    "    for j in range(len(y_pred_list[0][0])):\n",
    "        tmp.append(0)\n",
    "    y_pred.append(tmp)\n",
    "for k in range(len(y_pred_list)):\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[0])):\n",
    "            y_pred[i][j]+=y_pred_list[k][i][j]\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[0])):\n",
    "        if(y_pred[i][j]>=threshold):\n",
    "            y_pred[i][j]=1\n",
    "        else:\n",
    "            y_pred[i][j]=0\n",
    "y_pred_np = np.array(y_pred)\n",
    "score_list = y_test.T.tolist()\n",
    "predict_score_list = y_pred_np.T.tolist()\n",
    "evaluation(score_list,predict_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Chain 1\n",
      "Training Chain 2\n",
      "Training Chain 3\n",
      "Training Chain 4\n",
      "Training Chain 5\n",
      "Training Chain 6\n",
      "Training Chain 7\n",
      "Training Chain 8\n",
      "Training Chain 9\n",
      "Training Chain 10\n"
     ]
    }
   ],
   "source": [
    "base = SVC()\n",
    "\n",
    "chains = [ClassifierChain(base,order = 'random',random_state=i) for i in range(10)]\n",
    "cnt=1\n",
    "y_pred_list = []\n",
    "for chain in chains:\n",
    "    print(\"Training Chain \"+ str(cnt))\n",
    "    cnt+=1\n",
    "    chain.fit(x_train,y_train)\n",
    "    y_pred = chain.predict(x_test)\n",
    "    y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.17156632173095013\n",
      "Exact Prediction: 0.26575729068673565\n",
      "0.8206918438590923\n",
      "1.3607577055495423\n",
      "2.302551360308473\n",
      "3.2190240781776223\n",
      "4.182203790442923\n",
      "5.142849109435835\n",
      "6.002961969548695\n",
      "6.959633559898276\n",
      "Macro F-Score: 0.8699541949872845\n",
      "Micro F-Score: 0.8974845418774592\n",
      "Joy accuracy: 0.7342427093132643\n",
      "Sadness accuracy: 0.605832549388523\n",
      "Anger accuracy: 0.8904045155221072\n",
      "Disgust accuracy: 0.848071495766698\n",
      "Admiration accuracy: 0.9289746001881467\n",
      "Surprise accuracy: 0.9242709313264346\n",
      "Interest accuracy: 0.7784571966133584\n",
      "Fear accuracy: 0.9172154280338665\n",
      "Average Accuracy: 0.8284336782690498\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "threshold = 5\n",
    "for i in range(len(y_pred_list[0])):\n",
    "    tmp = []\n",
    "    for j in range(len(y_pred_list[0][0])):\n",
    "        tmp.append(0)\n",
    "    y_pred.append(tmp)\n",
    "for k in range(len(y_pred_list)):\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[0])):\n",
    "            y_pred[i][j]+=y_pred_list[k][i][j]\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[0])):\n",
    "        if(y_pred[i][j]>=threshold):\n",
    "            y_pred[i][j]=1\n",
    "        else:\n",
    "            y_pred[i][j]=0\n",
    "y_pred_np = np.array(y_pred)\n",
    "score_list = y_test.T.tolist()\n",
    "predict_score_list = y_pred_np.T.tolist()\n",
    "evaluation(score_list,predict_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
