{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>conoidal</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38949</th>\n",
       "      <td>web-based</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31365</th>\n",
       "      <td>seismic</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#fear</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>conductivity</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18631</th>\n",
       "      <td>investor</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12865</th>\n",
       "      <td>female</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9120</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11080</th>\n",
       "      <td>electropositive</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24522</th>\n",
       "      <td>ontogeny</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21990</th>\n",
       "      <td>microcarpa</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Word    Primary    Secondary Polarity\n",
       "7027          conoidal       #joy  #admiration    0.034\n",
       "38949        web-based       #joy    #interest    0.787\n",
       "31365          seismic   #sadness        #fear    -0.78\n",
       "6868      conductivity  #interest  #admiration    0.881\n",
       "18631         investor  #interest    #interest    0.076\n",
       "12865           female       #joy  #admiration    0.836\n",
       "9120       destabilize   #sadness       #anger    -0.72\n",
       "11080  electropositive       #joy  #admiration    0.666\n",
       "24522         ontogeny     #anger     #disgust    -0.75\n",
       "21990       microcarpa       #joy    #surprise    0.715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "8501\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "negatives = set(negatives)\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7425\n",
      "1    1076\n",
      "dtype: int64\n",
      "0    6043\n",
      "1    2458\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So much for sleeping in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so much sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College days are loooong days.. 3 more hours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>college day be loooong day more hour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@daihard I'm headed to Kentucky this time. Nev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i m head kentucky time never be be fun gqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hella tired.. where is gilbert for the usual b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hella tire be gilbert usual basketball talk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not as dry this morning as would have liked  l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not dry morning a have like lot moisture dune ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lil_laura_loo Really? I think we have some! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>laura loo really i think have i ve take pirite...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Do512_Kristin it's a good thing they give you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kristin s good thing give xanax something i ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PB&amp;amp;J, Owl City, and boredom.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pb amp j owl city boredom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So it's Saturday again &amp;amp; what do I do..? W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s saturday again amp do i do work again course</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trying to relax and watch Nascar, Difficult 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>try relax watch nascar difficult cause child d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0                          So much for sleeping in.     0        0      0   \n",
       "1      College days are loooong days.. 3 more hours     0        1      0   \n",
       "2  @daihard I'm headed to Kentucky this time. Nev...    0        0      0   \n",
       "3  hella tired.. where is gilbert for the usual b...    0        0      0   \n",
       "4  Not as dry this morning as would have liked  l...    0        1      0   \n",
       "5  @lil_laura_loo Really? I think we have some! I...    0        1      0   \n",
       "6  @Do512_Kristin it's a good thing they give you...    0        0      0   \n",
       "7                  PB&amp;J, Owl City, and boredom.     0        1      0   \n",
       "8  So it's Saturday again &amp; what do I do..? W...    0        1      0   \n",
       "9  trying to relax and watch Nascar, Difficult 'c...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         0     1   \n",
       "1        0           0         0         1     0   \n",
       "2        0           0         0         1     0   \n",
       "3        0           0         0         1     0   \n",
       "4        1           0         0         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        1           0         0         0     0   \n",
       "7        1           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        1           0         0         0     0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0                                     so much sleep       0       0  \n",
       "1              college day be loooong day more hour       0       0  \n",
       "2        i m head kentucky time never be be fun gqz       1       1  \n",
       "3       hella tire be gilbert usual basketball talk       1       1  \n",
       "4  not dry morning a have like lot moisture dune ...      0       1  \n",
       "5  laura loo really i think have i ve take pirite...      1       1  \n",
       "6  kristin s good thing give xanax something i ba...      0       0  \n",
       "7                         pb amp j owl city boredom       0       0  \n",
       "8    s saturday again amp do i do work again course       1       0  \n",
       "9  try relax watch nascar difficult cause child d...      0       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "X = df[['Analysed','qmark','exmark']]\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10952\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294, 8) (851, 8)\n",
      "Hamming Loss: 0.154524089306698\n",
      "Exact Prediction: 0.2796709753231492\n",
      "Macro F-Score: 0.8923793813785309\n",
      "Micro F-Score: 0.9093103448275863\n",
      "Average Accuracy: 0.8454759106933021\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286, 8) (850, 8)\n",
      "Hamming Loss: 0.1652941176470588\n",
      "Exact Prediction: 0.27176470588235296\n",
      "Macro F-Score: 0.887411801989982\n",
      "Micro F-Score: 0.9030533034328101\n",
      "Average Accuracy: 0.8347058823529411\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292, 8) (850, 8)\n",
      "Hamming Loss: 0.1648529411764706\n",
      "Exact Prediction: 0.24588235294117647\n",
      "Macro F-Score: 0.8873647005009703\n",
      "Micro F-Score: 0.9027838001907901\n",
      "Average Accuracy: 0.8351470588235295\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344, 8) (850, 8)\n",
      "Hamming Loss: 0.16058823529411764\n",
      "Exact Prediction: 0.26\n",
      "Macro F-Score: 0.8896274095452457\n",
      "Micro F-Score: 0.905634289664708\n",
      "Average Accuracy: 0.8394117647058824\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315, 8) (850, 8)\n",
      "Hamming Loss: 0.16955882352941176\n",
      "Exact Prediction: 0.24\n",
      "Macro F-Score: 0.8837915511775886\n",
      "Micro F-Score: 0.9009024495058015\n",
      "Average Accuracy: 0.8304411764705883\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326, 8) (850, 8)\n",
      "Hamming Loss: 0.1638235294117647\n",
      "Exact Prediction: 0.26235294117647057\n",
      "Macro F-Score: 0.8865986658975022\n",
      "Micro F-Score: 0.9038162666206181\n",
      "Average Accuracy: 0.8361764705882351\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305, 8) (850, 8)\n",
      "Hamming Loss: 0.15735294117647058\n",
      "Exact Prediction: 0.2541176470588235\n",
      "Macro F-Score: 0.892301982685422\n",
      "Micro F-Score: 0.9074874632543662\n",
      "Average Accuracy: 0.8426470588235294\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293, 8) (850, 8)\n",
      "Hamming Loss: 0.1585294117647059\n",
      "Exact Prediction: 0.26235294117647057\n",
      "Macro F-Score: 0.8920064316032468\n",
      "Micro F-Score: 0.9072767933941166\n",
      "Average Accuracy: 0.8414705882352942\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314, 8) (850, 8)\n",
      "Hamming Loss: 0.16588235294117648\n",
      "Exact Prediction: 0.2847058823529412\n",
      "Macro F-Score: 0.8858336296504377\n",
      "Micro F-Score: 0.9020493226814866\n",
      "Average Accuracy: 0.8341176470588235\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327, 8) (850, 8)\n",
      "Hamming Loss: 0.1610294117647059\n",
      "Exact Prediction: 0.2752941176470588\n",
      "Macro F-Score: 0.8899490717569061\n",
      "Micro F-Score: 0.9052522280868737\n",
      "Average Accuracy: 0.838970588235294\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.162143585401258\n",
      "Average Subset Accuracy: 0.26361415635584434\n",
      "Average Macro F-score: 0.8887264626185832\n",
      "Average Micro F-score: 0.9047566261659157\n",
      "Average of Average Accuracy: 0.8378564145987418\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].values.tolist(),Y.iloc[test_index].values.tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(tmp_list)\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(tmp_list2)\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    base = RandomForestClassifier()\n",
    "    chain_rfc = ClassifierChain(base,order = 'random',random_state=0)\n",
    "    chain_rfc.fit(x_train,y_train)\n",
    "    y_pred = chain_rfc.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.154524</td>\n",
       "      <td>0.279671</td>\n",
       "      <td>0.892379</td>\n",
       "      <td>0.909310</td>\n",
       "      <td>0.845476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.165294</td>\n",
       "      <td>0.271765</td>\n",
       "      <td>0.887412</td>\n",
       "      <td>0.903053</td>\n",
       "      <td>0.834706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.164853</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.887365</td>\n",
       "      <td>0.902784</td>\n",
       "      <td>0.835147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.160588</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.889627</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.839412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.883792</td>\n",
       "      <td>0.900902</td>\n",
       "      <td>0.830441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.163824</td>\n",
       "      <td>0.262353</td>\n",
       "      <td>0.886599</td>\n",
       "      <td>0.903816</td>\n",
       "      <td>0.836176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.157353</td>\n",
       "      <td>0.254118</td>\n",
       "      <td>0.892302</td>\n",
       "      <td>0.907487</td>\n",
       "      <td>0.842647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.158529</td>\n",
       "      <td>0.262353</td>\n",
       "      <td>0.892006</td>\n",
       "      <td>0.907277</td>\n",
       "      <td>0.841471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.165882</td>\n",
       "      <td>0.284706</td>\n",
       "      <td>0.885834</td>\n",
       "      <td>0.902049</td>\n",
       "      <td>0.834118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.161029</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>0.889949</td>\n",
       "      <td>0.905252</td>\n",
       "      <td>0.838971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.162144</td>\n",
       "      <td>0.263614</td>\n",
       "      <td>0.888726</td>\n",
       "      <td>0.904757</td>\n",
       "      <td>0.837856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.154524         0.279671       0.892379       0.909310   \n",
       "1         2      0.165294         0.271765       0.887412       0.903053   \n",
       "2         3      0.164853         0.245882       0.887365       0.902784   \n",
       "3         4      0.160588         0.260000       0.889627       0.905634   \n",
       "4         5      0.169559         0.240000       0.883792       0.900902   \n",
       "5         6      0.163824         0.262353       0.886599       0.903816   \n",
       "6         7      0.157353         0.254118       0.892302       0.907487   \n",
       "7         8      0.158529         0.262353       0.892006       0.907277   \n",
       "8         9      0.165882         0.284706       0.885834       0.902049   \n",
       "9        10      0.161029         0.275294       0.889949       0.905252   \n",
       "10  average      0.162144         0.263614       0.888726       0.904757   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.845476  \n",
       "1           0.834706  \n",
       "2           0.835147  \n",
       "3           0.839412  \n",
       "4           0.830441  \n",
       "5           0.836176  \n",
       "6           0.842647  \n",
       "7           0.841471  \n",
       "8           0.834118  \n",
       "9           0.838971  \n",
       "10          0.837856  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294, 8) (851, 8)\n",
      "Hamming Loss: 0.1636310223266745\n",
      "Exact Prediction: 0.28319623971797886\n",
      "Macro F-Score: 0.8772918105574208\n",
      "Micro F-Score: 0.9029109290569983\n",
      "Average Accuracy: 0.8363689776733254\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286, 8) (850, 8)\n",
      "Hamming Loss: 0.17558823529411766\n",
      "Exact Prediction: 0.29411764705882354\n",
      "Macro F-Score: 0.8724714753968911\n",
      "Micro F-Score: 0.8956293706293706\n",
      "Average Accuracy: 0.8244117647058823\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292, 8) (850, 8)\n",
      "Hamming Loss: 0.16661764705882354\n",
      "Exact Prediction: 0.2788235294117647\n",
      "Macro F-Score: 0.8802208610327236\n",
      "Micro F-Score: 0.9006576063130207\n",
      "Average Accuracy: 0.8333823529411764\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344, 8) (850, 8)\n",
      "Hamming Loss: 0.1676470588235294\n",
      "Exact Prediction: 0.2964705882352941\n",
      "Macro F-Score: 0.8778370737724617\n",
      "Micro F-Score: 0.9002100840336134\n",
      "Average Accuracy: 0.8323529411764706\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315, 8) (850, 8)\n",
      "Hamming Loss: 0.17691176470588235\n",
      "Exact Prediction: 0.2564705882352941\n",
      "Macro F-Score: 0.8701581501392049\n",
      "Micro F-Score: 0.8950353372306082\n",
      "Average Accuracy: 0.8230882352941177\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326, 8) (850, 8)\n",
      "Hamming Loss: 0.1713235294117647\n",
      "Exact Prediction: 0.27176470588235296\n",
      "Macro F-Score: 0.8744389123907822\n",
      "Micro F-Score: 0.8981732366051918\n",
      "Average Accuracy: 0.8286764705882352\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305, 8) (850, 8)\n",
      "Hamming Loss: 0.1688235294117647\n",
      "Exact Prediction: 0.2647058823529412\n",
      "Macro F-Score: 0.8757283271377093\n",
      "Micro F-Score: 0.8994041359971959\n",
      "Average Accuracy: 0.8311764705882353\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293, 8) (850, 8)\n",
      "Hamming Loss: 0.16852941176470587\n",
      "Exact Prediction: 0.2988235294117647\n",
      "Macro F-Score: 0.8768688742063364\n",
      "Micro F-Score: 0.9000174489617867\n",
      "Average Accuracy: 0.8314705882352941\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314, 8) (850, 8)\n",
      "Hamming Loss: 0.17279411764705882\n",
      "Exact Prediction: 0.2823529411764706\n",
      "Macro F-Score: 0.8731335872572892\n",
      "Micro F-Score: 0.8970111315628013\n",
      "Average Accuracy: 0.8272058823529411\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327, 8) (850, 8)\n",
      "Hamming Loss: 0.17647058823529413\n",
      "Exact Prediction: 0.2811764705882353\n",
      "Macro F-Score: 0.8682589429534752\n",
      "Micro F-Score: 0.8949211908931699\n",
      "Average Accuracy: 0.8235294117647058\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.17083369046796154\n",
      "Average Subset Accuracy: 0.280790212207092\n",
      "Average Macro F-score: 0.8746408014844294\n",
      "Average Micro F-score: 0.8983970471283756\n",
      "Average of Average Accuracy: 0.8291663095320383\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].values.tolist(),Y.iloc[test_index].values.tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(tmp_list)\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(tmp_list2)\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    base = SVC()\n",
    "    chain_rfc = ClassifierChain(base,order = 'random',random_state=0)\n",
    "    chain_rfc.fit(x_train,y_train)\n",
    "    y_pred = chain_rfc.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.163631</td>\n",
       "      <td>0.283196</td>\n",
       "      <td>0.877292</td>\n",
       "      <td>0.902911</td>\n",
       "      <td>0.836369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.175588</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.872471</td>\n",
       "      <td>0.895629</td>\n",
       "      <td>0.824412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.166618</td>\n",
       "      <td>0.278824</td>\n",
       "      <td>0.880221</td>\n",
       "      <td>0.900658</td>\n",
       "      <td>0.833382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.167647</td>\n",
       "      <td>0.296471</td>\n",
       "      <td>0.877837</td>\n",
       "      <td>0.900210</td>\n",
       "      <td>0.832353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.176912</td>\n",
       "      <td>0.256471</td>\n",
       "      <td>0.870158</td>\n",
       "      <td>0.895035</td>\n",
       "      <td>0.823088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.171324</td>\n",
       "      <td>0.271765</td>\n",
       "      <td>0.874439</td>\n",
       "      <td>0.898173</td>\n",
       "      <td>0.828676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.168824</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.875728</td>\n",
       "      <td>0.899404</td>\n",
       "      <td>0.831176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.168529</td>\n",
       "      <td>0.298824</td>\n",
       "      <td>0.876869</td>\n",
       "      <td>0.900017</td>\n",
       "      <td>0.831471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.172794</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>0.827206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.281176</td>\n",
       "      <td>0.868259</td>\n",
       "      <td>0.894921</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.170834</td>\n",
       "      <td>0.280790</td>\n",
       "      <td>0.874641</td>\n",
       "      <td>0.898397</td>\n",
       "      <td>0.829166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.163631         0.283196       0.877292       0.902911   \n",
       "1         2      0.175588         0.294118       0.872471       0.895629   \n",
       "2         3      0.166618         0.278824       0.880221       0.900658   \n",
       "3         4      0.167647         0.296471       0.877837       0.900210   \n",
       "4         5      0.176912         0.256471       0.870158       0.895035   \n",
       "5         6      0.171324         0.271765       0.874439       0.898173   \n",
       "6         7      0.168824         0.264706       0.875728       0.899404   \n",
       "7         8      0.168529         0.298824       0.876869       0.900017   \n",
       "8         9      0.172794         0.282353       0.873134       0.897011   \n",
       "9        10      0.176471         0.281176       0.868259       0.894921   \n",
       "10  average      0.170834         0.280790       0.874641       0.898397   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.836369  \n",
       "1           0.824412  \n",
       "2           0.833382  \n",
       "3           0.832353  \n",
       "4           0.823088  \n",
       "5           0.828676  \n",
       "6           0.831176  \n",
       "7           0.831471  \n",
       "8           0.827206  \n",
       "9           0.823529  \n",
       "10          0.829166  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
