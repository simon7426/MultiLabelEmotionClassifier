{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6095\n",
      "1    2406\n",
      "Name: Joy, dtype: int64\n",
      "0    4375\n",
      "1    4126\n",
      "Name: Sadness, dtype: int64\n",
      "0    7512\n",
      "1     989\n",
      "Name: Anger, dtype: int64\n",
      "0    7121\n",
      "1    1380\n",
      "Name: Disgust, dtype: int64\n",
      "0    7923\n",
      "1     578\n",
      "Name: Admiration, dtype: int64\n",
      "0    7877\n",
      "1     624\n",
      "Name: Surprise, dtype: int64\n",
      "0    6367\n",
      "1    2134\n",
      "Name: Interest, dtype: int64\n",
      "0    7827\n",
      "1     674\n",
      "Name: Fear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    print(df[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27742</th>\n",
       "      <td>proam</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24224</th>\n",
       "      <td>obovoid</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37824</th>\n",
       "      <td>unventilated</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>disablement</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30869</th>\n",
       "      <td>savorless</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32401</th>\n",
       "      <td>skinless</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#fear</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19968</th>\n",
       "      <td>leatherjacket</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>better-funded</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>bridle</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>chicest</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word      Primary    Secondary Polarity\n",
       "27742          proam     #sadness       #anger    -0.86\n",
       "24224        obovoid    #interest  #admiration    0.082\n",
       "37824   unventilated       #anger     #disgust    -0.72\n",
       "9589     disablement     #sadness     #disgust    -0.67\n",
       "30869      savorless     #sadness     #disgust    -0.74\n",
       "32401       skinless     #sadness        #fear    -0.78\n",
       "19968  leatherjacket       #anger     #disgust     -0.7\n",
       "3307   better-funded        #fear     #disgust    -0.80\n",
       "4244          bridle  #admiration  #admiration    0.071\n",
       "5620         chicest         #joy  #admiration    0.919"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "8501\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "negatives = set(negatives)\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7425\n",
      "1    1076\n",
      "dtype: int64\n",
      "0    6043\n",
      "1    2458\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So much for sleeping in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so much sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College days are loooong days.. 3 more hours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>college day be loooong day more hour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@daihard I'm headed to Kentucky this time. Nev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i m head kentucky time never be be fun gqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hella tired.. where is gilbert for the usual b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hella tire be gilbert usual basketball talk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not as dry this morning as would have liked  l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not dry morning a have like lot moisture dune ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lil_laura_loo Really? I think we have some! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>laura loo really i think have i ve take pirite...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Do512_Kristin it's a good thing they give you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kristin s good thing give xanax something i ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PB&amp;amp;J, Owl City, and boredom.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pb amp j owl city boredom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So it's Saturday again &amp;amp; what do I do..? W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s saturday again amp do i do work again course</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trying to relax and watch Nascar, Difficult 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>try relax watch nascar difficult cause child d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0                          So much for sleeping in.     0        0      0   \n",
       "1      College days are loooong days.. 3 more hours     0        1      0   \n",
       "2  @daihard I'm headed to Kentucky this time. Nev...    0        0      0   \n",
       "3  hella tired.. where is gilbert for the usual b...    0        0      0   \n",
       "4  Not as dry this morning as would have liked  l...    0        1      0   \n",
       "5  @lil_laura_loo Really? I think we have some! I...    0        1      0   \n",
       "6  @Do512_Kristin it's a good thing they give you...    0        0      0   \n",
       "7                  PB&amp;J, Owl City, and boredom.     0        1      0   \n",
       "8  So it's Saturday again &amp; what do I do..? W...    0        1      0   \n",
       "9  trying to relax and watch Nascar, Difficult 'c...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         0     1   \n",
       "1        0           0         0         1     0   \n",
       "2        0           0         0         1     0   \n",
       "3        0           0         0         1     0   \n",
       "4        1           0         0         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        1           0         0         0     0   \n",
       "7        1           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        1           0         0         0     0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0                                     so much sleep       0       0  \n",
       "1              college day be loooong day more hour       0       0  \n",
       "2        i m head kentucky time never be be fun gqz       1       1  \n",
       "3       hella tire be gilbert usual basketball talk       1       1  \n",
       "4  not dry morning a have like lot moisture dune ...      0       1  \n",
       "5  laura loo really i think have i ve take pirite...      1       1  \n",
       "6  kristin s good thing give xanax something i ba...      0       0  \n",
       "7                         pb amp j owl city boredom       0       0  \n",
       "8    s saturday again amp do i do work again course       1       0  \n",
       "9  try relax watch nascar difficult cause child d...      0       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "def convtodec(x):\n",
    "    val = 128\n",
    "    ret = 0\n",
    "    for y in x:\n",
    "        if(y):\n",
    "            ret+=val\n",
    "        val=val>>1\n",
    "    return ret\n",
    "labelpowerset = []\n",
    "for row in df.iterrows():\n",
    "    tmp = row[1][1:9].tolist()\n",
    "    labelpowerset.append(convtodec(tmp))\n",
    "print(len(labelpowerset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "cntpowerset = {}\n",
    "for val in labelpowerset:\n",
    "    if(val in cntpowerset):\n",
    "        cntpowerset[val]+=1\n",
    "    else:\n",
    "        cntpowerset[val]=1\n",
    "\n",
    "cntpowersetlist = []\n",
    "for key,val in cntpowerset.items():\n",
    "    cntpowersetlist.append((val,key))\n",
    "cntpowersetlist.sort(reverse=True)\n",
    "print(len(cntpowersetlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {}\n",
    "revlabel = {}\n",
    "cnt = 0\n",
    "for val in cntpowersetlist:\n",
    "    label[val[1]] = cnt\n",
    "    revlabel[cnt] = val[1]\n",
    "    cnt+=1\n",
    "powset = [label[x] for x in labelpowerset]\n",
    "df['powerset'] = powset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1870\n",
       "1      998\n",
       "2      772\n",
       "3      711\n",
       "4      348\n",
       "      ... \n",
       "85       1\n",
       "93       1\n",
       "86       1\n",
       "94       1\n",
       "91       1\n",
       "Name: powerset, Length: 95, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['powerset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df_tmp = df\n",
    "print(len(df_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tmp[['Analysed','qmark','exmark']]\n",
    "Y = df_tmp['powerset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10952\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tmp['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2bin(xx):\n",
    "    ret = []\n",
    "    for i in range(8):\n",
    "        if(xx & (1<<i)):\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    #print(xx)\n",
    "    ret.reverse()\n",
    "    #print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294,) (851,)\n",
      "Hamming Loss: 0.16084018801410105\n",
      "Exact Prediction: 0.28672150411280845\n",
      "Macro F-Score: 0.8826201527247391\n",
      "Micro F-Score: 0.9046748498302429\n",
      "Average Accuracy: 0.839159811985899\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286,) (850,)\n",
      "Hamming Loss: 0.17176470588235293\n",
      "Exact Prediction: 0.28352941176470586\n",
      "Macro F-Score: 0.8768186500754755\n",
      "Micro F-Score: 0.8980268901693731\n",
      "Average Accuracy: 0.8282352941176471\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292,) (850,)\n",
      "Hamming Loss: 0.1698529411764706\n",
      "Exact Prediction: 0.26235294117647057\n",
      "Macro F-Score: 0.8792282701823101\n",
      "Micro F-Score: 0.8987996144747218\n",
      "Average Accuracy: 0.8301470588235293\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344,) (850,)\n",
      "Hamming Loss: 0.1713235294117647\n",
      "Exact Prediction: 0.2752941176470588\n",
      "Macro F-Score: 0.8758400841510378\n",
      "Micro F-Score: 0.8980484816662292\n",
      "Average Accuracy: 0.8286764705882353\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315,) (850,)\n",
      "Hamming Loss: 0.17220588235294118\n",
      "Exact Prediction: 0.27294117647058824\n",
      "Macro F-Score: 0.8766237334796344\n",
      "Micro F-Score: 0.8978274147107581\n",
      "Average Accuracy: 0.8277941176470588\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326,) (850,)\n",
      "Hamming Loss: 0.16955882352941176\n",
      "Exact Prediction: 0.2752941176470588\n",
      "Macro F-Score: 0.8766100403412825\n",
      "Micro F-Score: 0.899222095970632\n",
      "Average Accuracy: 0.8304411764705883\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305,) (850,)\n",
      "Hamming Loss: 0.16779411764705882\n",
      "Exact Prediction: 0.2647058823529412\n",
      "Macro F-Score: 0.8775947066132646\n",
      "Micro F-Score: 0.8999035003070445\n",
      "Average Accuracy: 0.8322058823529412\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293,) (850,)\n",
      "Hamming Loss: 0.17161764705882354\n",
      "Exact Prediction: 0.27647058823529413\n",
      "Macro F-Score: 0.8751852824221372\n",
      "Micro F-Score: 0.8981941900026171\n",
      "Average Accuracy: 0.8283823529411766\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314,) (850,)\n",
      "Hamming Loss: 0.16720588235294118\n",
      "Exact Prediction: 0.28941176470588237\n",
      "Macro F-Score: 0.8781608475069443\n",
      "Micro F-Score: 0.9002719059731602\n",
      "Average Accuracy: 0.8327941176470588\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327,) (850,)\n",
      "Hamming Loss: 0.16794117647058823\n",
      "Exact Prediction: 0.28352941176470586\n",
      "Macro F-Score: 0.8773216844283505\n",
      "Micro F-Score: 0.9000874890638669\n",
      "Average Accuracy: 0.8320588235294117\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.1690104893896454\n",
      "Average Subset Accuracy: 0.27702509158775146\n",
      "Average Macro F-score: 0.8776003451925176\n",
      "Average Micro F-score: 0.8995056432168648\n",
      "Average of Average Accuracy: 0.8309895106103546\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = RandomForestClassifier(n_estimators=300)\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.160840</td>\n",
       "      <td>0.286722</td>\n",
       "      <td>0.882620</td>\n",
       "      <td>0.904675</td>\n",
       "      <td>0.839160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.171765</td>\n",
       "      <td>0.283529</td>\n",
       "      <td>0.876819</td>\n",
       "      <td>0.898027</td>\n",
       "      <td>0.828235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.169853</td>\n",
       "      <td>0.262353</td>\n",
       "      <td>0.879228</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>0.830147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.171324</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>0.875840</td>\n",
       "      <td>0.898048</td>\n",
       "      <td>0.828676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.172206</td>\n",
       "      <td>0.272941</td>\n",
       "      <td>0.876624</td>\n",
       "      <td>0.897827</td>\n",
       "      <td>0.827794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>0.876610</td>\n",
       "      <td>0.899222</td>\n",
       "      <td>0.830441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.167794</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.877595</td>\n",
       "      <td>0.899904</td>\n",
       "      <td>0.832206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.171618</td>\n",
       "      <td>0.276471</td>\n",
       "      <td>0.875185</td>\n",
       "      <td>0.898194</td>\n",
       "      <td>0.828382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.167206</td>\n",
       "      <td>0.289412</td>\n",
       "      <td>0.878161</td>\n",
       "      <td>0.900272</td>\n",
       "      <td>0.832794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.167941</td>\n",
       "      <td>0.283529</td>\n",
       "      <td>0.877322</td>\n",
       "      <td>0.900087</td>\n",
       "      <td>0.832059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.169010</td>\n",
       "      <td>0.277025</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>0.899506</td>\n",
       "      <td>0.830990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.160840         0.286722       0.882620       0.904675   \n",
       "1         2      0.171765         0.283529       0.876819       0.898027   \n",
       "2         3      0.169853         0.262353       0.879228       0.898800   \n",
       "3         4      0.171324         0.275294       0.875840       0.898048   \n",
       "4         5      0.172206         0.272941       0.876624       0.897827   \n",
       "5         6      0.169559         0.275294       0.876610       0.899222   \n",
       "6         7      0.167794         0.264706       0.877595       0.899904   \n",
       "7         8      0.171618         0.276471       0.875185       0.898194   \n",
       "8         9      0.167206         0.289412       0.878161       0.900272   \n",
       "9        10      0.167941         0.283529       0.877322       0.900087   \n",
       "10  average      0.169010         0.277025       0.877600       0.899506   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.839160  \n",
       "1           0.828235  \n",
       "2           0.830147  \n",
       "3           0.828676  \n",
       "4           0.827794  \n",
       "5           0.830441  \n",
       "6           0.832206  \n",
       "7           0.828382  \n",
       "8           0.832794  \n",
       "9           0.832059  \n",
       "10          0.830990  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294,) (851,)\n",
      "Hamming Loss: 0.1640716803760282\n",
      "Exact Prediction: 0.28672150411280845\n",
      "Macro F-Score: 0.8705730076733853\n",
      "Micro F-Score: 0.902640983177896\n",
      "Average Accuracy: 0.8359283196239717\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286,) (850,)\n",
      "Hamming Loss: 0.17985294117647058\n",
      "Exact Prediction: 0.28\n",
      "Macro F-Score: 0.8639945157083814\n",
      "Micro F-Score: 0.8931037496722314\n",
      "Average Accuracy: 0.8201470588235293\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292,) (850,)\n",
      "Hamming Loss: 0.17352941176470588\n",
      "Exact Prediction: 0.26\n",
      "Macro F-Score: 0.8689889231690886\n",
      "Micro F-Score: 0.8965275341985268\n",
      "Average Accuracy: 0.8264705882352941\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344,) (850,)\n",
      "Hamming Loss: 0.17102941176470587\n",
      "Exact Prediction: 0.29058823529411765\n",
      "Macro F-Score: 0.867787364539929\n",
      "Micro F-Score: 0.8982235057320381\n",
      "Average Accuracy: 0.828970588235294\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315,) (850,)\n",
      "Hamming Loss: 0.1713235294117647\n",
      "Exact Prediction: 0.26705882352941174\n",
      "Macro F-Score: 0.8699126447323458\n",
      "Micro F-Score: 0.8983863933711296\n",
      "Average Accuracy: 0.8286764705882352\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326,) (850,)\n",
      "Hamming Loss: 0.17176470588235293\n",
      "Exact Prediction: 0.28352941176470586\n",
      "Macro F-Score: 0.8676310580709954\n",
      "Micro F-Score: 0.8978842454974646\n",
      "Average Accuracy: 0.828235294117647\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305,) (850,)\n",
      "Hamming Loss: 0.1738235294117647\n",
      "Exact Prediction: 0.24941176470588236\n",
      "Macro F-Score: 0.8668451979565266\n",
      "Micro F-Score: 0.8963884992987378\n",
      "Average Accuracy: 0.8261764705882352\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293,) (850,)\n",
      "Hamming Loss: 0.1701470588235294\n",
      "Exact Prediction: 0.29764705882352943\n",
      "Macro F-Score: 0.870637934174789\n",
      "Micro F-Score: 0.8990313290863077\n",
      "Average Accuracy: 0.8298529411764706\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314,) (850,)\n",
      "Hamming Loss: 0.17455882352941177\n",
      "Exact Prediction: 0.2823529411764706\n",
      "Macro F-Score: 0.8668829679684855\n",
      "Micro F-Score: 0.8959410888051196\n",
      "Average Accuracy: 0.8254411764705883\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327,) (850,)\n",
      "Hamming Loss: 0.17411764705882352\n",
      "Exact Prediction: 0.28941176470588237\n",
      "Macro F-Score: 0.8633427202933159\n",
      "Micro F-Score: 0.8963040812751796\n",
      "Average Accuracy: 0.8258823529411764\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.17242187391995573\n",
      "Average Subset Accuracy: 0.27867215041128085\n",
      "Average Macro F-score: 0.8676596334287243\n",
      "Average Micro F-score: 0.8974431410114632\n",
      "Average of Average Accuracy: 0.8275781260800441\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = SVC()\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.164072</td>\n",
       "      <td>0.286722</td>\n",
       "      <td>0.870573</td>\n",
       "      <td>0.902641</td>\n",
       "      <td>0.835928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.179853</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.863995</td>\n",
       "      <td>0.893104</td>\n",
       "      <td>0.820147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.173529</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.868989</td>\n",
       "      <td>0.896528</td>\n",
       "      <td>0.826471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.171029</td>\n",
       "      <td>0.290588</td>\n",
       "      <td>0.867787</td>\n",
       "      <td>0.898224</td>\n",
       "      <td>0.828971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.171324</td>\n",
       "      <td>0.267059</td>\n",
       "      <td>0.869913</td>\n",
       "      <td>0.898386</td>\n",
       "      <td>0.828676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.171765</td>\n",
       "      <td>0.283529</td>\n",
       "      <td>0.867631</td>\n",
       "      <td>0.897884</td>\n",
       "      <td>0.828235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>0.249412</td>\n",
       "      <td>0.866845</td>\n",
       "      <td>0.896388</td>\n",
       "      <td>0.826176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.170147</td>\n",
       "      <td>0.297647</td>\n",
       "      <td>0.870638</td>\n",
       "      <td>0.899031</td>\n",
       "      <td>0.829853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.174559</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.895941</td>\n",
       "      <td>0.825441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.289412</td>\n",
       "      <td>0.863343</td>\n",
       "      <td>0.896304</td>\n",
       "      <td>0.825882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.172422</td>\n",
       "      <td>0.278672</td>\n",
       "      <td>0.867660</td>\n",
       "      <td>0.897443</td>\n",
       "      <td>0.827578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.164072         0.286722       0.870573       0.902641   \n",
       "1         2      0.179853         0.280000       0.863995       0.893104   \n",
       "2         3      0.173529         0.260000       0.868989       0.896528   \n",
       "3         4      0.171029         0.290588       0.867787       0.898224   \n",
       "4         5      0.171324         0.267059       0.869913       0.898386   \n",
       "5         6      0.171765         0.283529       0.867631       0.897884   \n",
       "6         7      0.173824         0.249412       0.866845       0.896388   \n",
       "7         8      0.170147         0.297647       0.870638       0.899031   \n",
       "8         9      0.174559         0.282353       0.866883       0.895941   \n",
       "9        10      0.174118         0.289412       0.863343       0.896304   \n",
       "10  average      0.172422         0.278672       0.867660       0.897443   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.835928  \n",
       "1           0.820147  \n",
       "2           0.826471  \n",
       "3           0.828971  \n",
       "4           0.828676  \n",
       "5           0.828235  \n",
       "6           0.826176  \n",
       "7           0.829853  \n",
       "8           0.825441  \n",
       "9           0.825882  \n",
       "10          0.827578  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
