{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9415</th>\n",
       "      <td>dieux</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>churchmanship</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>confessor</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16334</th>\n",
       "      <td>home-bound</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15211</th>\n",
       "      <td>grumpily</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#fear</td>\n",
       "      <td>-0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30095</th>\n",
       "      <td>right</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32384</th>\n",
       "      <td>skilfully</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19538</th>\n",
       "      <td>komitee</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>glycerine</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26503</th>\n",
       "      <td>picklepuss</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word    Primary    Secondary Polarity\n",
       "9415           dieux   #sadness       #anger    -0.57\n",
       "5859   churchmanship  #surprise  #admiration    0.583\n",
       "6888       confessor       #joy  #admiration    0.909\n",
       "16334     home-bound   #sadness     #disgust    -0.84\n",
       "15211       grumpily   #sadness        #fear    -0.32\n",
       "30095          right       #joy    #interest    0.926\n",
       "32384      skilfully  #surprise  #admiration    0.811\n",
       "19538        komitee       #joy  #admiration    0.723\n",
       "14715      glycerine  #interest  #admiration    0.047\n",
       "26503     picklepuss   #sadness       #anger    -0.73"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "8501\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "negatives = set(negatives)\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So much for sleeping in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so much sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College days are loooong days.. 3 more hours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>college day be loooong day more hour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@daihard I'm headed to Kentucky this time. Nev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i m head kentucky time never be be fun gqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hella tired.. where is gilbert for the usual b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hella tire be gilbert usual basketball talk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not as dry this morning as would have liked  l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not dry morning a have like lot moisture dune ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lil_laura_loo Really? I think we have some! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>laura loo really i think have i ve take pirite...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Do512_Kristin it's a good thing they give you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kristin s good thing give xanax something i ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PB&amp;amp;J, Owl City, and boredom.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pb amp j owl city boredom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So it's Saturday again &amp;amp; what do I do..? W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s saturday again amp do i do work again course</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trying to relax and watch Nascar, Difficult 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>try relax watch nascar difficult cause child d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0                          So much for sleeping in.     0        0      0   \n",
       "1      College days are loooong days.. 3 more hours     0        1      0   \n",
       "2  @daihard I'm headed to Kentucky this time. Nev...    0        0      0   \n",
       "3  hella tired.. where is gilbert for the usual b...    0        0      0   \n",
       "4  Not as dry this morning as would have liked  l...    0        1      0   \n",
       "5  @lil_laura_loo Really? I think we have some! I...    0        1      0   \n",
       "6  @Do512_Kristin it's a good thing they give you...    0        0      0   \n",
       "7                  PB&amp;J, Owl City, and boredom.     0        1      0   \n",
       "8  So it's Saturday again &amp; what do I do..? W...    0        1      0   \n",
       "9  trying to relax and watch Nascar, Difficult 'c...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         0     1   \n",
       "1        0           0         0         1     0   \n",
       "2        0           0         0         1     0   \n",
       "3        0           0         0         1     0   \n",
       "4        1           0         0         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        1           0         0         0     0   \n",
       "7        1           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        1           0         0         0     0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0                                     so much sleep       0       0  \n",
       "1              college day be loooong day more hour       0       0  \n",
       "2        i m head kentucky time never be be fun gqz       1       1  \n",
       "3       hella tire be gilbert usual basketball talk       1       1  \n",
       "4  not dry morning a have like lot moisture dune ...      0       1  \n",
       "5  laura loo really i think have i ve take pirite...      1       1  \n",
       "6  kristin s good thing give xanax something i ba...      0       0  \n",
       "7                         pb amp j owl city boredom       0       0  \n",
       "8    s saturday again amp do i do work again course       1       0  \n",
       "9  try relax watch nascar difficult cause child d...      0       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "X = df[['Analysed','qmark','exmark']]\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10952\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294, 8) (851, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1489424206815511\n",
      "Exact Prediction: 0.24206815511163338\n",
      "Macro F-Score: 0.9013396492751129\n",
      "Micro F-Score: 0.9138487680543755\n",
      "Average Accuracy: 0.8510575793184488\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16294117647058823\n",
      "Exact Prediction: 0.23176470588235293\n",
      "Macro F-Score: 0.8951863386623182\n",
      "Micro F-Score: 0.9058623619371282\n",
      "Average Accuracy: 0.8370588235294119\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.15970588235294117\n",
      "Exact Prediction: 0.2211764705882353\n",
      "Macro F-Score: 0.8965691789594459\n",
      "Micro F-Score: 0.9072270630445924\n",
      "Average Accuracy: 0.8402941176470589\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16411764705882353\n",
      "Exact Prediction: 0.20941176470588235\n",
      "Macro F-Score: 0.8920921289568089\n",
      "Micro F-Score: 0.9047293836435035\n",
      "Average Accuracy: 0.8358823529411765\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16308823529411764\n",
      "Exact Prediction: 0.20823529411764705\n",
      "Macro F-Score: 0.8939906104734344\n",
      "Micro F-Score: 0.9058174097664542\n",
      "Average Accuracy: 0.8369117647058825\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16338235294117648\n",
      "Exact Prediction: 0.2211764705882353\n",
      "Macro F-Score: 0.893192907638131\n",
      "Micro F-Score: 0.9055191768007484\n",
      "Average Accuracy: 0.8366176470588236\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.15661764705882353\n",
      "Exact Prediction: 0.23176470588235293\n",
      "Macro F-Score: 0.8973595170643189\n",
      "Micro F-Score: 0.9090132422041862\n",
      "Average Accuracy: 0.8433823529411765\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.15823529411764706\n",
      "Exact Prediction: 0.22941176470588234\n",
      "Macro F-Score: 0.8972797446638684\n",
      "Micro F-Score: 0.9084878380676986\n",
      "Average Accuracy: 0.841764705882353\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16176470588235295\n",
      "Exact Prediction: 0.25882352941176473\n",
      "Macro F-Score: 0.8932933292793713\n",
      "Micro F-Score: 0.9056765563368204\n",
      "Average Accuracy: 0.838235294117647\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1661764705882353\n",
      "Exact Prediction: 0.18352941176470589\n",
      "Macro F-Score: 0.8921020812635103\n",
      "Micro F-Score: 0.9040095141012572\n",
      "Average Accuracy: 0.8338235294117646\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.16049718324462572\n",
      "Average Subset Accuracy: 0.22373622727586925\n",
      "Average Macro F-score: 0.8952405486236321\n",
      "Average Micro F-score: 0.9070191313956766\n",
      "Average of Average Accuracy: 0.8395028167553743\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].values.tolist(),Y.iloc[test_index].values.tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(tmp_list)\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(tmp_list2)\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    y_pred_val = []\n",
    "    for i in range(8):\n",
    "        print(i)\n",
    "        classifier = RandomForestClassifier(n_estimators=300)\n",
    "        tmp_y_train = y_train[:,i]\n",
    "        classifier.fit(x_train,tmp_y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        y_pred_val.append(y_pred)\n",
    "    \n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.148942</td>\n",
       "      <td>0.242068</td>\n",
       "      <td>0.901340</td>\n",
       "      <td>0.913849</td>\n",
       "      <td>0.851058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.162941</td>\n",
       "      <td>0.231765</td>\n",
       "      <td>0.895186</td>\n",
       "      <td>0.905862</td>\n",
       "      <td>0.837059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.159706</td>\n",
       "      <td>0.221176</td>\n",
       "      <td>0.896569</td>\n",
       "      <td>0.907227</td>\n",
       "      <td>0.840294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.164118</td>\n",
       "      <td>0.209412</td>\n",
       "      <td>0.892092</td>\n",
       "      <td>0.904729</td>\n",
       "      <td>0.835882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.208235</td>\n",
       "      <td>0.893991</td>\n",
       "      <td>0.905817</td>\n",
       "      <td>0.836912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.163382</td>\n",
       "      <td>0.221176</td>\n",
       "      <td>0.893193</td>\n",
       "      <td>0.905519</td>\n",
       "      <td>0.836618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.156618</td>\n",
       "      <td>0.231765</td>\n",
       "      <td>0.897360</td>\n",
       "      <td>0.909013</td>\n",
       "      <td>0.843382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.229412</td>\n",
       "      <td>0.897280</td>\n",
       "      <td>0.908488</td>\n",
       "      <td>0.841765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.905677</td>\n",
       "      <td>0.838235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.166176</td>\n",
       "      <td>0.183529</td>\n",
       "      <td>0.892102</td>\n",
       "      <td>0.904010</td>\n",
       "      <td>0.833824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.160497</td>\n",
       "      <td>0.223736</td>\n",
       "      <td>0.895241</td>\n",
       "      <td>0.907019</td>\n",
       "      <td>0.839503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.148942         0.242068       0.901340       0.913849   \n",
       "1         2      0.162941         0.231765       0.895186       0.905862   \n",
       "2         3      0.159706         0.221176       0.896569       0.907227   \n",
       "3         4      0.164118         0.209412       0.892092       0.904729   \n",
       "4         5      0.163088         0.208235       0.893991       0.905817   \n",
       "5         6      0.163382         0.221176       0.893193       0.905519   \n",
       "6         7      0.156618         0.231765       0.897360       0.909013   \n",
       "7         8      0.158235         0.229412       0.897280       0.908488   \n",
       "8         9      0.161765         0.258824       0.893293       0.905677   \n",
       "9        10      0.166176         0.183529       0.892102       0.904010   \n",
       "10  average      0.160497         0.223736       0.895241       0.907019   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.851058  \n",
       "1           0.837059  \n",
       "2           0.840294  \n",
       "3           0.835882  \n",
       "4           0.836912  \n",
       "5           0.836618  \n",
       "6           0.843382  \n",
       "7           0.841765  \n",
       "8           0.838235  \n",
       "9           0.833824  \n",
       "10          0.839503  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(9294, 10954) (851, 10954)\n",
      "(9294, 8) (851, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.15481786133960046\n",
      "Exact Prediction: 0.21739130434782608\n",
      "Macro F-Score: 0.8971152285093467\n",
      "Micro F-Score: 0.9107233610028799\n",
      "Average Accuracy: 0.8451821386603995\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(9286, 10954) (850, 10954)\n",
      "(9286, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16470588235294117\n",
      "Exact Prediction: 0.2164705882352941\n",
      "Macro F-Score: 0.8928940016696837\n",
      "Micro F-Score: 0.9049720006787715\n",
      "Average Accuracy: 0.8352941176470587\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(9292, 10954) (850, 10954)\n",
      "(9292, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16411764705882353\n",
      "Exact Prediction: 0.20470588235294118\n",
      "Macro F-Score: 0.8930808773181346\n",
      "Micro F-Score: 0.9048268804366366\n",
      "Average Accuracy: 0.8358823529411764\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(9344, 10954) (850, 10954)\n",
      "(9344, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1610294117647059\n",
      "Exact Prediction: 0.21176470588235294\n",
      "Macro F-Score: 0.8943242726263905\n",
      "Micro F-Score: 0.9069589599796074\n",
      "Average Accuracy: 0.8389705882352941\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(9315, 10954) (850, 10954)\n",
      "(9315, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16455882352941176\n",
      "Exact Prediction: 0.1976470588235294\n",
      "Macro F-Score: 0.8919001530142142\n",
      "Micro F-Score: 0.905161454360539\n",
      "Average Accuracy: 0.8354411764705882\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(9326, 10954) (850, 10954)\n",
      "(9326, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1625\n",
      "Exact Prediction: 0.20823529411764705\n",
      "Macro F-Score: 0.8937996457425111\n",
      "Micro F-Score: 0.906347995592847\n",
      "Average Accuracy: 0.8374999999999999\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(9305, 10954) (850, 10954)\n",
      "(9305, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1589705882352941\n",
      "Exact Prediction: 0.2164705882352941\n",
      "Macro F-Score: 0.8956148469603735\n",
      "Micro F-Score: 0.9078038379530916\n",
      "Average Accuracy: 0.8410294117647057\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(9293, 10954) (850, 10954)\n",
      "(9293, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.1585294117647059\n",
      "Exact Prediction: 0.22470588235294117\n",
      "Macro F-Score: 0.8975308033872955\n",
      "Micro F-Score: 0.9087986463620982\n",
      "Average Accuracy: 0.841470588235294\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(9314, 10954) (850, 10954)\n",
      "(9314, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16014705882352942\n",
      "Exact Prediction: 0.2376470588235294\n",
      "Macro F-Score: 0.8959053481193548\n",
      "Micro F-Score: 0.90727969348659\n",
      "Average Accuracy: 0.8398529411764706\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(9327, 10954) (850, 10954)\n",
      "(9327, 8) (850, 8)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Hamming Loss: 0.16176470588235295\n",
      "Exact Prediction: 0.2\n",
      "Macro F-Score: 0.8947716189031597\n",
      "Micro F-Score: 0.9067322367305409\n",
      "Average Accuracy: 0.838235294117647\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.16111413907513653\n",
      "Average Subset Accuracy: 0.21350383631713554\n",
      "Average Macro F-score: 0.8946936796250465\n",
      "Average Micro F-score: 0.9069605066583601\n",
      "Average of Average Accuracy: 0.8388858609248633\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].values.tolist(),Y.iloc[test_index].values.tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(tmp_list)\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(tmp_list2)\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    y_pred_val = []\n",
    "    for i in range(8):\n",
    "        print(i)\n",
    "        classifier = SVC()\n",
    "        tmp_y_train = y_train[:,i]\n",
    "        classifier.fit(x_train,tmp_y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        y_pred_val.append(y_pred)\n",
    "    \n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.154818</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.897115</td>\n",
       "      <td>0.910723</td>\n",
       "      <td>0.845182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.216471</td>\n",
       "      <td>0.892894</td>\n",
       "      <td>0.904972</td>\n",
       "      <td>0.835294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.164118</td>\n",
       "      <td>0.204706</td>\n",
       "      <td>0.893081</td>\n",
       "      <td>0.904827</td>\n",
       "      <td>0.835882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.161029</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.894324</td>\n",
       "      <td>0.906959</td>\n",
       "      <td>0.838971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.164559</td>\n",
       "      <td>0.197647</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.905161</td>\n",
       "      <td>0.835441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.208235</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.906348</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.158971</td>\n",
       "      <td>0.216471</td>\n",
       "      <td>0.895615</td>\n",
       "      <td>0.907804</td>\n",
       "      <td>0.841029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.158529</td>\n",
       "      <td>0.224706</td>\n",
       "      <td>0.897531</td>\n",
       "      <td>0.908799</td>\n",
       "      <td>0.841471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.160147</td>\n",
       "      <td>0.237647</td>\n",
       "      <td>0.895905</td>\n",
       "      <td>0.907280</td>\n",
       "      <td>0.839853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.894772</td>\n",
       "      <td>0.906732</td>\n",
       "      <td>0.838235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.213504</td>\n",
       "      <td>0.894694</td>\n",
       "      <td>0.906961</td>\n",
       "      <td>0.838886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.154818         0.217391       0.897115       0.910723   \n",
       "1         2      0.164706         0.216471       0.892894       0.904972   \n",
       "2         3      0.164118         0.204706       0.893081       0.904827   \n",
       "3         4      0.161029         0.211765       0.894324       0.906959   \n",
       "4         5      0.164559         0.197647       0.891900       0.905161   \n",
       "5         6      0.162500         0.208235       0.893800       0.906348   \n",
       "6         7      0.158971         0.216471       0.895615       0.907804   \n",
       "7         8      0.158529         0.224706       0.897531       0.908799   \n",
       "8         9      0.160147         0.237647       0.895905       0.907280   \n",
       "9        10      0.161765         0.200000       0.894772       0.906732   \n",
       "10  average      0.161114         0.213504       0.894694       0.906961   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.845182  \n",
       "1           0.835294  \n",
       "2           0.835882  \n",
       "3           0.838971  \n",
       "4           0.835441  \n",
       "5           0.837500  \n",
       "6           0.841029  \n",
       "7           0.841471  \n",
       "8           0.839853  \n",
       "9           0.838235  \n",
       "10          0.838886  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
