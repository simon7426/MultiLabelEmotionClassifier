{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25030</th>\n",
       "      <td>overcome</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463</th>\n",
       "      <td>unmarried</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14788</th>\n",
       "      <td>goldfish</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#anger</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9387</th>\n",
       "      <td>dictatorship</td>\n",
       "      <td>#fear</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12383</th>\n",
       "      <td>extension</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18492</th>\n",
       "      <td>intimidation</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23129</th>\n",
       "      <td>myotonia</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31560</th>\n",
       "      <td>senile</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#fear</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19134</th>\n",
       "      <td>judicious</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36829</th>\n",
       "      <td>unapproachable</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word    Primary    Secondary Polarity\n",
       "25030        overcome       #joy  #admiration    0.881\n",
       "37463       unmarried   #sadness     #disgust    -0.81\n",
       "14788        goldfish   #sadness       #anger    -0.78\n",
       "9387     dictatorship      #fear     #disgust    -0.75\n",
       "12383       extension  #interest  #admiration    0.161\n",
       "18492    intimidation     #anger     #disgust    -0.87\n",
       "23129        myotonia     #anger     #disgust    -0.68\n",
       "31560          senile   #sadness        #fear    -0.87\n",
       "19134       judicious  #surprise  #admiration    0.762\n",
       "36829  unapproachable   #sadness     #disgust    -0.94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "X = df['Text']\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17402\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Text'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(7650, 17402) (851, 17402)\n",
      "(7650, 8) (851, 8)\n",
      "Hamming Loss: 0.16304347826086957\n",
      "Exact Prediction: 0.23971797884841364\n",
      "Macro F-Score: 0.8846109921881393\n",
      "Micro F-Score: 0.9051282051282051\n",
      "Average Accuracy: 0.8369565217391305\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1686764705882353\n",
      "Exact Prediction: 0.23411764705882354\n",
      "Macro F-Score: 0.8850478335708134\n",
      "Micro F-Score: 0.9016210652714641\n",
      "Average Accuracy: 0.8313235294117647\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1698529411764706\n",
      "Exact Prediction: 0.20352941176470588\n",
      "Macro F-Score: 0.880914505296437\n",
      "Micro F-Score: 0.8999740192257729\n",
      "Average Accuracy: 0.8301470588235293\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1673529411764706\n",
      "Exact Prediction: 0.2576470588235294\n",
      "Macro F-Score: 0.8673787748501733\n",
      "Micro F-Score: 0.9004722756690572\n",
      "Average Accuracy: 0.8326470588235294\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16970588235294118\n",
      "Exact Prediction: 0.2411764705882353\n",
      "Macro F-Score: 0.8786913099556348\n",
      "Micro F-Score: 0.9005172413793102\n",
      "Average Accuracy: 0.8302941176470587\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17147058823529412\n",
      "Exact Prediction: 0.25058823529411767\n",
      "Macro F-Score: 0.8691711144828567\n",
      "Micro F-Score: 0.898854961832061\n",
      "Average Accuracy: 0.8285294117647058\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17073529411764707\n",
      "Exact Prediction: 0.2635294117647059\n",
      "Macro F-Score: 0.8680294431427372\n",
      "Micro F-Score: 0.8983451536643028\n",
      "Average Accuracy: 0.8292647058823529\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16926470588235293\n",
      "Exact Prediction: 0.27058823529411763\n",
      "Macro F-Score: 0.8804961266230593\n",
      "Micro F-Score: 0.9006645378441356\n",
      "Average Accuracy: 0.8307352941176469\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16823529411764707\n",
      "Exact Prediction: 0.21411764705882352\n",
      "Macro F-Score: 0.8894668115724845\n",
      "Micro F-Score: 0.9029850746268657\n",
      "Average Accuracy: 0.831764705882353\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1688235294117647\n",
      "Exact Prediction: 0.22823529411764706\n",
      "Macro F-Score: 0.8820133159140682\n",
      "Micro F-Score: 0.9009148972898325\n",
      "Average Accuracy: 0.8311764705882353\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.1687161125319693\n",
      "Average Subset Accuracy: 0.24032473906131196\n",
      "Average Macro F-score: 0.8785820227596405\n",
      "Average Micro F-score: 0.9009477431931006\n",
      "Average of Average Accuracy: 0.8312838874680308\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = np.array(Y.iloc[train_index].values.tolist()),np.array(Y.iloc[test_index].values.tolist())\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    classifier = RakelD(base_classifier = RandomForestClassifier(),\n",
    "                        base_classifier_require_dense=[False, False],\n",
    "                        labelset_size=4)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred.toarray()\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.239718</td>\n",
       "      <td>0.884611</td>\n",
       "      <td>0.905128</td>\n",
       "      <td>0.836957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.168676</td>\n",
       "      <td>0.234118</td>\n",
       "      <td>0.885048</td>\n",
       "      <td>0.901621</td>\n",
       "      <td>0.831324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.169853</td>\n",
       "      <td>0.203529</td>\n",
       "      <td>0.880915</td>\n",
       "      <td>0.899974</td>\n",
       "      <td>0.830147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.167353</td>\n",
       "      <td>0.257647</td>\n",
       "      <td>0.867379</td>\n",
       "      <td>0.900472</td>\n",
       "      <td>0.832647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.169706</td>\n",
       "      <td>0.241176</td>\n",
       "      <td>0.878691</td>\n",
       "      <td>0.900517</td>\n",
       "      <td>0.830294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.171471</td>\n",
       "      <td>0.250588</td>\n",
       "      <td>0.869171</td>\n",
       "      <td>0.898855</td>\n",
       "      <td>0.828529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.170735</td>\n",
       "      <td>0.263529</td>\n",
       "      <td>0.868029</td>\n",
       "      <td>0.898345</td>\n",
       "      <td>0.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.169265</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.880496</td>\n",
       "      <td>0.900665</td>\n",
       "      <td>0.830735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.168235</td>\n",
       "      <td>0.214118</td>\n",
       "      <td>0.889467</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.831765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.168824</td>\n",
       "      <td>0.228235</td>\n",
       "      <td>0.882013</td>\n",
       "      <td>0.900915</td>\n",
       "      <td>0.831176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.168716</td>\n",
       "      <td>0.240325</td>\n",
       "      <td>0.878582</td>\n",
       "      <td>0.900948</td>\n",
       "      <td>0.831284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.163043         0.239718       0.884611       0.905128   \n",
       "1         2      0.168676         0.234118       0.885048       0.901621   \n",
       "2         3      0.169853         0.203529       0.880915       0.899974   \n",
       "3         4      0.167353         0.257647       0.867379       0.900472   \n",
       "4         5      0.169706         0.241176       0.878691       0.900517   \n",
       "5         6      0.171471         0.250588       0.869171       0.898855   \n",
       "6         7      0.170735         0.263529       0.868029       0.898345   \n",
       "7         8      0.169265         0.270588       0.880496       0.900665   \n",
       "8         9      0.168235         0.214118       0.889467       0.902985   \n",
       "9        10      0.168824         0.228235       0.882013       0.900915   \n",
       "10  average      0.168716         0.240325       0.878582       0.900948   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.836957  \n",
       "1           0.831324  \n",
       "2           0.830147  \n",
       "3           0.832647  \n",
       "4           0.830294  \n",
       "5           0.828529  \n",
       "6           0.829265  \n",
       "7           0.830735  \n",
       "8           0.831765  \n",
       "9           0.831176  \n",
       "10          0.831284  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(7650, 17402) (851, 17402)\n",
      "(7650, 8) (851, 8)\n",
      "Hamming Loss: 0.1605464159811986\n",
      "Exact Prediction: 0.2291421856639248\n",
      "Macro F-Score: 0.8855431645248196\n",
      "Micro F-Score: 0.9062848323758896\n",
      "Average Accuracy: 0.8394535840188014\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17\n",
      "Exact Prediction: 0.18352941176470589\n",
      "Macro F-Score: 0.8906761367523182\n",
      "Micro F-Score: 0.9026772183869338\n",
      "Average Accuracy: 0.83\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1711764705882353\n",
      "Exact Prediction: 0.22588235294117648\n",
      "Macro F-Score: 0.8624944774127281\n",
      "Micro F-Score: 0.8978230337078651\n",
      "Average Accuracy: 0.8288235294117647\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1698529411764706\n",
      "Exact Prediction: 0.26235294117647057\n",
      "Macro F-Score: 0.8612303301039919\n",
      "Micro F-Score: 0.8991002009260067\n",
      "Average Accuracy: 0.8301470588235293\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17573529411764705\n",
      "Exact Prediction: 0.24352941176470588\n",
      "Macro F-Score: 0.8556922344468203\n",
      "Micro F-Score: 0.8957515484602634\n",
      "Average Accuracy: 0.8242647058823529\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1610294117647059\n",
      "Exact Prediction: 0.18941176470588236\n",
      "Macro F-Score: 0.8973606373127753\n",
      "Micro F-Score: 0.9081914982812106\n",
      "Average Accuracy: 0.8389705882352942\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16441176470588234\n",
      "Exact Prediction: 0.23294117647058823\n",
      "Macro F-Score: 0.8787529840476926\n",
      "Micro F-Score: 0.9029345372460497\n",
      "Average Accuracy: 0.8355882352941177\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17176470588235293\n",
      "Exact Prediction: 0.2776470588235294\n",
      "Macro F-Score: 0.86078842865771\n",
      "Micro F-Score: 0.8981158408932309\n",
      "Average Accuracy: 0.8282352941176471\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1673529411764706\n",
      "Exact Prediction: 0.2011764705882353\n",
      "Macro F-Score: 0.8910215776792572\n",
      "Micro F-Score: 0.9037876225904634\n",
      "Average Accuracy: 0.8326470588235294\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16779411764705882\n",
      "Exact Prediction: 0.24\n",
      "Macro F-Score: 0.8739889713861697\n",
      "Micro F-Score: 0.9011008061021063\n",
      "Average Accuracy: 0.8322058823529412\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.16796640630400222\n",
      "Average Subset Accuracy: 0.22856127738992188\n",
      "Average Macro F-score: 0.8757548942324282\n",
      "Average Micro F-score: 0.901576713897002\n",
      "Average of Average Accuracy: 0.8320335936959978\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = np.array(Y.iloc[train_index].values.tolist()),np.array(Y.iloc[test_index].values.tolist())\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    classifier = RakelD(base_classifier = SVC(),\n",
    "                        base_classifier_require_dense=[False, False],\n",
    "                        labelset_size=4)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred.toarray()\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.160546</td>\n",
       "      <td>0.229142</td>\n",
       "      <td>0.885543</td>\n",
       "      <td>0.906285</td>\n",
       "      <td>0.839454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.183529</td>\n",
       "      <td>0.890676</td>\n",
       "      <td>0.902677</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.225882</td>\n",
       "      <td>0.862494</td>\n",
       "      <td>0.897823</td>\n",
       "      <td>0.828824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.169853</td>\n",
       "      <td>0.262353</td>\n",
       "      <td>0.861230</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.830147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.175735</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.855692</td>\n",
       "      <td>0.895752</td>\n",
       "      <td>0.824265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.161029</td>\n",
       "      <td>0.189412</td>\n",
       "      <td>0.897361</td>\n",
       "      <td>0.908191</td>\n",
       "      <td>0.838971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.164412</td>\n",
       "      <td>0.232941</td>\n",
       "      <td>0.878753</td>\n",
       "      <td>0.902935</td>\n",
       "      <td>0.835588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.171765</td>\n",
       "      <td>0.277647</td>\n",
       "      <td>0.860788</td>\n",
       "      <td>0.898116</td>\n",
       "      <td>0.828235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.167353</td>\n",
       "      <td>0.201176</td>\n",
       "      <td>0.891022</td>\n",
       "      <td>0.903788</td>\n",
       "      <td>0.832647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.167794</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.873989</td>\n",
       "      <td>0.901101</td>\n",
       "      <td>0.832206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.167966</td>\n",
       "      <td>0.228561</td>\n",
       "      <td>0.875755</td>\n",
       "      <td>0.901577</td>\n",
       "      <td>0.832034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.160546         0.229142       0.885543       0.906285   \n",
       "1         2      0.170000         0.183529       0.890676       0.902677   \n",
       "2         3      0.171176         0.225882       0.862494       0.897823   \n",
       "3         4      0.169853         0.262353       0.861230       0.899100   \n",
       "4         5      0.175735         0.243529       0.855692       0.895752   \n",
       "5         6      0.161029         0.189412       0.897361       0.908191   \n",
       "6         7      0.164412         0.232941       0.878753       0.902935   \n",
       "7         8      0.171765         0.277647       0.860788       0.898116   \n",
       "8         9      0.167353         0.201176       0.891022       0.903788   \n",
       "9        10      0.167794         0.240000       0.873989       0.901101   \n",
       "10  average      0.167966         0.228561       0.875755       0.901577   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.839454  \n",
       "1           0.830000  \n",
       "2           0.828824  \n",
       "3           0.830147  \n",
       "4           0.824265  \n",
       "5           0.838971  \n",
       "6           0.835588  \n",
       "7           0.828235  \n",
       "8           0.832647  \n",
       "9           0.832206  \n",
       "10          0.832034  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
