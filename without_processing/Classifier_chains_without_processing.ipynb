{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "X = df['Text']\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17402\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Text'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(7650, 17402) (851, 17402)\n",
      "(7650, 8) (851, 8)\n",
      "Hamming Loss: 0.15804935370152762\n",
      "Exact Prediction: 0.23266745005875442\n",
      "Macro F-Score: 0.8905738777158825\n",
      "Micro F-Score: 0.9083475298126065\n",
      "Average Accuracy: 0.8419506462984724\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16970588235294118\n",
      "Exact Prediction: 0.2376470588235294\n",
      "Macro F-Score: 0.885050084861993\n",
      "Micro F-Score: 0.9013000342114267\n",
      "Average Accuracy: 0.8302941176470588\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16661764705882354\n",
      "Exact Prediction: 0.20588235294117646\n",
      "Macro F-Score: 0.8876694207992332\n",
      "Micro F-Score: 0.9029550321199143\n",
      "Average Accuracy: 0.8333823529411765\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1625\n",
      "Exact Prediction: 0.2376470588235294\n",
      "Macro F-Score: 0.8886385325435959\n",
      "Micro F-Score: 0.9053046533550432\n",
      "Average Accuracy: 0.8374999999999999\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16455882352941176\n",
      "Exact Prediction: 0.23176470588235293\n",
      "Macro F-Score: 0.8862071735572685\n",
      "Micro F-Score: 0.9043834914124582\n",
      "Average Accuracy: 0.8354411764705882\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1623529411764706\n",
      "Exact Prediction: 0.23529411764705882\n",
      "Macro F-Score: 0.8893360648480337\n",
      "Micro F-Score: 0.9058181197747825\n",
      "Average Accuracy: 0.8376470588235295\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.15823529411764706\n",
      "Exact Prediction: 0.2211764705882353\n",
      "Macro F-Score: 0.8925366673315848\n",
      "Micro F-Score: 0.9078924841636706\n",
      "Average Accuracy: 0.8417647058823529\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16514705882352942\n",
      "Exact Prediction: 0.24823529411764705\n",
      "Macro F-Score: 0.8862190149940427\n",
      "Micro F-Score: 0.9040744853506449\n",
      "Average Accuracy: 0.8348529411764707\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16161764705882353\n",
      "Exact Prediction: 0.25058823529411767\n",
      "Macro F-Score: 0.8889920635268902\n",
      "Micro F-Score: 0.9057218838466157\n",
      "Average Accuracy: 0.8383823529411765\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16852941176470587\n",
      "Exact Prediction: 0.20705882352941177\n",
      "Macro F-Score: 0.8848793042982189\n",
      "Micro F-Score: 0.9020680225602462\n",
      "Average Accuracy: 0.8314705882352942\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.16373140595838803\n",
      "Average Subset Accuracy: 0.23079615677058132\n",
      "Average Macro F-score: 0.8880102204476744\n",
      "Average Micro F-score: 0.904786573660741\n",
      "Average of Average Accuracy: 0.8362685940416121\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = np.array(Y.iloc[train_index].values.tolist()),np.array(Y.iloc[test_index].values.tolist())\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    \n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    base = RandomForestClassifier()\n",
    "    chain_rfc = ClassifierChain(base,order = 'random',random_state=0)\n",
    "    chain_rfc.fit(x_train,y_train)\n",
    "    y_pred = chain_rfc.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.158049</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>0.890574</td>\n",
       "      <td>0.908348</td>\n",
       "      <td>0.841951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.169706</td>\n",
       "      <td>0.237647</td>\n",
       "      <td>0.885050</td>\n",
       "      <td>0.901300</td>\n",
       "      <td>0.830294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.166618</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.887669</td>\n",
       "      <td>0.902955</td>\n",
       "      <td>0.833382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.237647</td>\n",
       "      <td>0.888639</td>\n",
       "      <td>0.905305</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.164559</td>\n",
       "      <td>0.231765</td>\n",
       "      <td>0.886207</td>\n",
       "      <td>0.904383</td>\n",
       "      <td>0.835441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.162353</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.889336</td>\n",
       "      <td>0.905818</td>\n",
       "      <td>0.837647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.221176</td>\n",
       "      <td>0.892537</td>\n",
       "      <td>0.907892</td>\n",
       "      <td>0.841765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.165147</td>\n",
       "      <td>0.248235</td>\n",
       "      <td>0.886219</td>\n",
       "      <td>0.904074</td>\n",
       "      <td>0.834853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.161618</td>\n",
       "      <td>0.250588</td>\n",
       "      <td>0.888992</td>\n",
       "      <td>0.905722</td>\n",
       "      <td>0.838382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.168529</td>\n",
       "      <td>0.207059</td>\n",
       "      <td>0.884879</td>\n",
       "      <td>0.902068</td>\n",
       "      <td>0.831471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.163731</td>\n",
       "      <td>0.230796</td>\n",
       "      <td>0.888010</td>\n",
       "      <td>0.904787</td>\n",
       "      <td>0.836269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.158049         0.232667       0.890574       0.908348   \n",
       "1         2      0.169706         0.237647       0.885050       0.901300   \n",
       "2         3      0.166618         0.205882       0.887669       0.902955   \n",
       "3         4      0.162500         0.237647       0.888639       0.905305   \n",
       "4         5      0.164559         0.231765       0.886207       0.904383   \n",
       "5         6      0.162353         0.235294       0.889336       0.905818   \n",
       "6         7      0.158235         0.221176       0.892537       0.907892   \n",
       "7         8      0.165147         0.248235       0.886219       0.904074   \n",
       "8         9      0.161618         0.250588       0.888992       0.905722   \n",
       "9        10      0.168529         0.207059       0.884879       0.902068   \n",
       "10  average      0.163731         0.230796       0.888010       0.904787   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.841951  \n",
       "1           0.830294  \n",
       "2           0.833382  \n",
       "3           0.837500  \n",
       "4           0.835441  \n",
       "5           0.837647  \n",
       "6           0.841765  \n",
       "7           0.834853  \n",
       "8           0.838382  \n",
       "9           0.831471  \n",
       "10          0.836269  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(7650, 17402) (851, 17402)\n",
      "(7650, 8) (851, 8)\n",
      "Hamming Loss: 0.17332549941245592\n",
      "Exact Prediction: 0.2573443008225617\n",
      "Macro F-Score: 0.8646406009952573\n",
      "Micro F-Score: 0.8972304476572026\n",
      "Average Accuracy: 0.8266745005875441\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17852941176470588\n",
      "Exact Prediction: 0.2635294117647059\n",
      "Macro F-Score: 0.8651785082012032\n",
      "Micro F-Score: 0.8939923157527069\n",
      "Average Accuracy: 0.8214705882352942\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17720588235294119\n",
      "Exact Prediction: 0.24352941176470588\n",
      "Macro F-Score: 0.8669359807890754\n",
      "Micro F-Score: 0.8942518648530058\n",
      "Average Accuracy: 0.8227941176470588\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17676470588235293\n",
      "Exact Prediction: 0.2611764705882353\n",
      "Macro F-Score: 0.867705061576679\n",
      "Micro F-Score: 0.8948013302993174\n",
      "Average Accuracy: 0.8232352941176471\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.18191176470588236\n",
      "Exact Prediction: 0.24235294117647058\n",
      "Macro F-Score: 0.861949893228158\n",
      "Micro F-Score: 0.8919933641840566\n",
      "Average Accuracy: 0.8180882352941177\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1723529411764706\n",
      "Exact Prediction: 0.26588235294117646\n",
      "Macro F-Score: 0.8692637524918786\n",
      "Micro F-Score: 0.897606150620304\n",
      "Average Accuracy: 0.8276470588235294\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17441176470588235\n",
      "Exact Prediction: 0.24588235294117647\n",
      "Macro F-Score: 0.8682045298034484\n",
      "Micro F-Score: 0.8961107217939733\n",
      "Average Accuracy: 0.8255882352941177\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16911764705882354\n",
      "Exact Prediction: 0.2988235294117647\n",
      "Macro F-Score: 0.8731626919268533\n",
      "Micro F-Score: 0.899633443882004\n",
      "Average Accuracy: 0.8308823529411765\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1763235294117647\n",
      "Exact Prediction: 0.2647058823529412\n",
      "Macro F-Score: 0.8657418259854649\n",
      "Micro F-Score: 0.8949075291436585\n",
      "Average Accuracy: 0.8236764705882353\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.17647058823529413\n",
      "Exact Prediction: 0.26705882352941174\n",
      "Macro F-Score: 0.8660809111202272\n",
      "Micro F-Score: 0.8949947497374868\n",
      "Average Accuracy: 0.8235294117647058\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.17564137347065736\n",
      "Average Subset Accuracy: 0.26102854772931494\n",
      "Average Macro F-score: 0.8668863756118246\n",
      "Average Micro F-score: 0.8955521917923714\n",
      "Average of Average Accuracy: 0.8243586265293427\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = np.array(Y.iloc[train_index].values.tolist()),np.array(Y.iloc[test_index].values.tolist())\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    \n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    base = SVC()\n",
    "    chain_rfc = ClassifierChain(base,order = 'random',random_state=0)\n",
    "    chain_rfc.fit(x_train,y_train)\n",
    "    y_pred = chain_rfc.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.173325</td>\n",
       "      <td>0.257344</td>\n",
       "      <td>0.864641</td>\n",
       "      <td>0.897230</td>\n",
       "      <td>0.826675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.178529</td>\n",
       "      <td>0.263529</td>\n",
       "      <td>0.865179</td>\n",
       "      <td>0.893992</td>\n",
       "      <td>0.821471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.177206</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.866936</td>\n",
       "      <td>0.894252</td>\n",
       "      <td>0.822794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.176765</td>\n",
       "      <td>0.261176</td>\n",
       "      <td>0.867705</td>\n",
       "      <td>0.894801</td>\n",
       "      <td>0.823235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.181912</td>\n",
       "      <td>0.242353</td>\n",
       "      <td>0.861950</td>\n",
       "      <td>0.891993</td>\n",
       "      <td>0.818088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.172353</td>\n",
       "      <td>0.265882</td>\n",
       "      <td>0.869264</td>\n",
       "      <td>0.897606</td>\n",
       "      <td>0.827647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.174412</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.868205</td>\n",
       "      <td>0.896111</td>\n",
       "      <td>0.825588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>0.298824</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.899633</td>\n",
       "      <td>0.830882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.176324</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.865742</td>\n",
       "      <td>0.894908</td>\n",
       "      <td>0.823676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.267059</td>\n",
       "      <td>0.866081</td>\n",
       "      <td>0.894995</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.175641</td>\n",
       "      <td>0.261029</td>\n",
       "      <td>0.866886</td>\n",
       "      <td>0.895552</td>\n",
       "      <td>0.824359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.173325         0.257344       0.864641       0.897230   \n",
       "1         2      0.178529         0.263529       0.865179       0.893992   \n",
       "2         3      0.177206         0.243529       0.866936       0.894252   \n",
       "3         4      0.176765         0.261176       0.867705       0.894801   \n",
       "4         5      0.181912         0.242353       0.861950       0.891993   \n",
       "5         6      0.172353         0.265882       0.869264       0.897606   \n",
       "6         7      0.174412         0.245882       0.868205       0.896111   \n",
       "7         8      0.169118         0.298824       0.873163       0.899633   \n",
       "8         9      0.176324         0.264706       0.865742       0.894908   \n",
       "9        10      0.176471         0.267059       0.866081       0.894995   \n",
       "10  average      0.175641         0.261029       0.866886       0.895552   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.826675  \n",
       "1           0.821471  \n",
       "2           0.822794  \n",
       "3           0.823235  \n",
       "4           0.818088  \n",
       "5           0.827647  \n",
       "6           0.825588  \n",
       "7           0.830882  \n",
       "8           0.823676  \n",
       "9           0.823529  \n",
       "10          0.824359  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
