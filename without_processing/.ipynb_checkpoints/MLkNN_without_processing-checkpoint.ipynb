{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from skmultilearn.adapt import MLkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "X = df['Text']\n",
    "Y = df[['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17402\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Text'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(7650, 17402) (851, 17402)\n",
      "(7650, 8) (851, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rainmaker\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.1649529964747356\n",
      "Exact Prediction: 0.19858989424206816\n",
      "Macro F-Score: 0.8878776319856899\n",
      "Micro F-Score: 0.9040744853506448\n",
      "Average Accuracy: 0.8350470035252644\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1698529411764706\n",
      "Exact Prediction: 0.21529411764705883\n",
      "Macro F-Score: 0.8860076788586857\n",
      "Micro F-Score: 0.9008498583569406\n",
      "Average Accuracy: 0.8301470588235295\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1711764705882353\n",
      "Exact Prediction: 0.2011764705882353\n",
      "Macro F-Score: 0.8859350828572398\n",
      "Micro F-Score: 0.8997416020671835\n",
      "Average Accuracy: 0.8288235294117646\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16426470588235295\n",
      "Exact Prediction: 0.21058823529411766\n",
      "Macro F-Score: 0.8895947652068781\n",
      "Micro F-Score: 0.9041941847499785\n",
      "Average Accuracy: 0.835735294117647\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16691176470588234\n",
      "Exact Prediction: 0.2023529411764706\n",
      "Macro F-Score: 0.8883267378064157\n",
      "Micro F-Score: 0.9030825719409104\n",
      "Average Accuracy: 0.8330882352941177\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1652941176470588\n",
      "Exact Prediction: 0.2023529411764706\n",
      "Macro F-Score: 0.891722249209099\n",
      "Micro F-Score: 0.9040300546448088\n",
      "Average Accuracy: 0.8347058823529413\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16544117647058823\n",
      "Exact Prediction: 0.20470588235294118\n",
      "Macro F-Score: 0.8889662289791674\n",
      "Micro F-Score: 0.9032424529113271\n",
      "Average Accuracy: 0.8345588235294118\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.16588235294117648\n",
      "Exact Prediction: 0.2164705882352941\n",
      "Macro F-Score: 0.8899470187042161\n",
      "Micro F-Score: 0.9034907597535933\n",
      "Average Accuracy: 0.8341176470588235\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1675\n",
      "Exact Prediction: 0.2235294117647059\n",
      "Macro F-Score: 0.8877894535619972\n",
      "Micro F-Score: 0.9023072304657347\n",
      "Average Accuracy: 0.8325000000000001\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(7651, 17402) (850, 17402)\n",
      "(7651, 8) (850, 8)\n",
      "Hamming Loss: 0.1652941176470588\n",
      "Exact Prediction: 0.2\n",
      "Macro F-Score: 0.8889438009632793\n",
      "Micro F-Score: 0.9035689773507207\n",
      "Average Accuracy: 0.8347058823529412\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.16665706435335592\n",
      "Average Subset Accuracy: 0.20750604824773622\n",
      "Average Macro F-score: 0.8885110648132668\n",
      "Average Micro F-score: 0.9028582177591842\n",
      "Average of Average Accuracy: 0.8333429356466441\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = np.array(Y.iloc[train_index].values.tolist()),np.array(Y.iloc[test_index].values.tolist())\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    \n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    classifier = MLkNN(k=100)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    y_pred_val = y_pred.toarray()\n",
    "    y_test_val = np.array(y_test)\n",
    "    \n",
    "    score_list = y_test_val.T.tolist()\n",
    "    predict_score_list = y_pred_val.T.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.164953</td>\n",
       "      <td>0.198590</td>\n",
       "      <td>0.887878</td>\n",
       "      <td>0.904074</td>\n",
       "      <td>0.835047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.169853</td>\n",
       "      <td>0.215294</td>\n",
       "      <td>0.886008</td>\n",
       "      <td>0.900850</td>\n",
       "      <td>0.830147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.201176</td>\n",
       "      <td>0.885935</td>\n",
       "      <td>0.899742</td>\n",
       "      <td>0.828824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.164265</td>\n",
       "      <td>0.210588</td>\n",
       "      <td>0.889595</td>\n",
       "      <td>0.904194</td>\n",
       "      <td>0.835735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.166912</td>\n",
       "      <td>0.202353</td>\n",
       "      <td>0.888327</td>\n",
       "      <td>0.903083</td>\n",
       "      <td>0.833088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.165294</td>\n",
       "      <td>0.202353</td>\n",
       "      <td>0.891722</td>\n",
       "      <td>0.904030</td>\n",
       "      <td>0.834706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.165441</td>\n",
       "      <td>0.204706</td>\n",
       "      <td>0.888966</td>\n",
       "      <td>0.903242</td>\n",
       "      <td>0.834559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.165882</td>\n",
       "      <td>0.216471</td>\n",
       "      <td>0.889947</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.834118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.887789</td>\n",
       "      <td>0.902307</td>\n",
       "      <td>0.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.165294</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.888944</td>\n",
       "      <td>0.903569</td>\n",
       "      <td>0.834706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.166657</td>\n",
       "      <td>0.207506</td>\n",
       "      <td>0.888511</td>\n",
       "      <td>0.902858</td>\n",
       "      <td>0.833343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.164953         0.198590       0.887878       0.904074   \n",
       "1         2      0.169853         0.215294       0.886008       0.900850   \n",
       "2         3      0.171176         0.201176       0.885935       0.899742   \n",
       "3         4      0.164265         0.210588       0.889595       0.904194   \n",
       "4         5      0.166912         0.202353       0.888327       0.903083   \n",
       "5         6      0.165294         0.202353       0.891722       0.904030   \n",
       "6         7      0.165441         0.204706       0.888966       0.903242   \n",
       "7         8      0.165882         0.216471       0.889947       0.903491   \n",
       "8         9      0.167500         0.223529       0.887789       0.902307   \n",
       "9        10      0.165294         0.200000       0.888944       0.903569   \n",
       "10  average      0.166657         0.207506       0.888511       0.902858   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.835047  \n",
       "1           0.830147  \n",
       "2           0.828824  \n",
       "3           0.835735  \n",
       "4           0.833088  \n",
       "5           0.834706  \n",
       "6           0.834559  \n",
       "7           0.834118  \n",
       "8           0.832500  \n",
       "9           0.834706  \n",
       "10          0.833343  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
