{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from senticnet5 import senticnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from scipy import sparse\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('hand8_k_random.xlsx')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6095\n",
      "1    2406\n",
      "Name: Joy, dtype: int64\n",
      "0    4375\n",
      "1    4126\n",
      "Name: Sadness, dtype: int64\n",
      "0    7512\n",
      "1     989\n",
      "Name: Anger, dtype: int64\n",
      "0    7121\n",
      "1    1380\n",
      "Name: Disgust, dtype: int64\n",
      "0    7923\n",
      "1     578\n",
      "Name: Admiration, dtype: int64\n",
      "0    7877\n",
      "1     624\n",
      "Name: Surprise, dtype: int64\n",
      "0    6367\n",
      "1    2134\n",
      "Name: Interest, dtype: int64\n",
      "0    7827\n",
      "1     674\n",
      "Name: Fear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for cl in df.columns:\n",
    "    if(cl=='Text'):\n",
    "        continue\n",
    "    print(df[cl].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>categorize</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>easygoing</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32324</th>\n",
       "      <td>situation</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32373</th>\n",
       "      <td>sketchy</td>\n",
       "      <td>#surprise</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12247</th>\n",
       "      <td>exotic</td>\n",
       "      <td>#joy</td>\n",
       "      <td>#interest</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16720</th>\n",
       "      <td>hydra-headed</td>\n",
       "      <td>#anger</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15882</th>\n",
       "      <td>heck</td>\n",
       "      <td>#sadness</td>\n",
       "      <td>#disgust</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24570</th>\n",
       "      <td>operator</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28974</th>\n",
       "      <td>reappoint</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30276</th>\n",
       "      <td>roman</td>\n",
       "      <td>#interest</td>\n",
       "      <td>#admiration</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word    Primary    Secondary Polarity\n",
       "5168     categorize   #sadness     #disgust    -0.88\n",
       "10796     easygoing       #joy  #admiration     0.78\n",
       "32324     situation  #interest  #admiration    0.783\n",
       "32373       sketchy  #surprise  #admiration    0.944\n",
       "12247        exotic       #joy    #interest    0.773\n",
       "16720  hydra-headed     #anger     #disgust    -0.97\n",
       "15882          heck   #sadness     #disgust    -0.15\n",
       "24570      operator  #interest  #admiration    0.142\n",
       "28974     reappoint  #interest  #admiration    0.788\n",
       "30276         roman  #interest  #admiration    0.021"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleword=[]\n",
    "for key,val in senticnet.items():\n",
    "    if(len(key.split('_'))==1):\n",
    "        singleword.append(key)\n",
    "print(len(singleword))\n",
    "word=[]\n",
    "primary=[]\n",
    "sec=[]\n",
    "pola=[]\n",
    "for x in singleword:\n",
    "    word.append(x)\n",
    "    primary.append(senticnet[x][4])\n",
    "    sec.append(senticnet[x][5])\n",
    "    pola.append(senticnet[x][7])\n",
    "df_emo=pd.DataFrame(list(zip(word,primary,sec,pola)),columns=[\"Word\",\"Primary\",\"Secondary\",\"Polarity\"])\n",
    "df_emo.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_count_user_mentions(tweet):\n",
    "    tweet_mentions_removed = re.subn(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = tweet_mentions_removed[0]\n",
    "    no_user_mentions = tweet_mentions_removed[1]\n",
    "    return tweet,no_user_mentions\n",
    "#%%\n",
    "def remove_count_urls(tweet):\n",
    "    tweet_url_removed = re.subn('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = tweet_url_removed[0]\n",
    "    no_urls = tweet_url_removed[1]\n",
    "    return tweet,no_urls\n",
    "#%%\n",
    "def remove_count_hashtags(tweet):\n",
    "    no_hashtags = len({tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")})\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    return tweet,no_hashtags    \n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = [\"J\",\"N\",\"V\",\"R\"]\n",
    "#need = [\"V\"]\n",
    "neg = [\"n't\",\"not\"]\n",
    "punct = [\".\",\",\",\"?\",\";\",\"!\"]\n",
    "opposite = {}\n",
    "opposite[\"#joy\"] = \"#sadness\"\n",
    "opposite[0] = 1\n",
    "opposite[\"#sadness\"] = \"#joy\"\n",
    "opposite[1] = 0\n",
    "opposite[\"#admiration\"] = \"#anger\"\n",
    "opposite[4] = 2\n",
    "opposite[\"#anger\"] = \"#admiration\"\n",
    "opposite[2] = 4\n",
    "opposite[\"#surprise\"] = \"#fear\"\n",
    "opposite[5] = 7\n",
    "opposite[\"#fear\"] = \"#surprise\"\n",
    "opposite[7] = 5\n",
    "opposite[\"#interest\"] = \"#disgust\"\n",
    "opposite[6] = 3\n",
    "opposite[\"#disgust\"] = \"#interest\"\n",
    "opposite[3] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "def normal_algo(sen):\n",
    "    NEGATION_ADVERBS = [\"no\", \"without\", \"nil\",\"not\", \"n't\", \"never\", \"none\", \"neith\", \"nor\", \"non\"]\n",
    "    NEGATION_VERBS = [\"deny\", \"reject\", \"refuse\", \"subside\", \"retract\", \"non\"]\n",
    "    CONJUCTION_WORDS = [\"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\"]\n",
    "    sen = sen.lower()\n",
    "    sen,removed_user_cnt = remove_count_user_mentions(sen)\n",
    "    sen,removed_url_cnt = remove_count_urls(sen)\n",
    "    sen,removed_hashtag_cnt = remove_count_hashtags(sen)\n",
    "    #print(sen)\n",
    "    tokens = word_tokenize(sen)\n",
    "    lem = [wordnet_lemmatizer.lemmatize(t,get_pos(t)) for t in tokens]\n",
    "    #print(lem)\n",
    "    lem_lookup = {}\n",
    "    for i in range(len(tokens)):\n",
    "        lem_lookup[tokens[i]]=lem[i]\n",
    "    mark_neg = {}\n",
    "    nflag = False\n",
    "    for t in lem:\n",
    "        if(t[0] in punct or t in CONJUCTION_WORDS):\n",
    "            nflag=False\n",
    "        if(nflag==True):\n",
    "            mark_neg[t]=1\n",
    "            negatives.append(t)\n",
    "        if(t in NEGATION_ADVERBS or t in NEGATION_VERBS):\n",
    "            nflag=True\n",
    "    tag1 = pos_tag(tokens)\n",
    "    #print(tag1)\n",
    "    tokens.clear()\n",
    "    for x in tag1:\n",
    "        #print(x)\n",
    "        if(x[1][0] in need):\n",
    "            tokens.append(x[0])\n",
    "    val = {}\n",
    "    #print(tokens)\n",
    "    ret_str = \"\"\n",
    "    for t in tokens:\n",
    "        t=lem_lookup[t]\n",
    "        ret_str+=t\n",
    "        ret_str+=\" \"\n",
    "        \"\"\"\n",
    "        if(t in senticnet):\n",
    "            x = senticnet[t][4]\n",
    "            #print(t)\n",
    "            if(t in mark_neg):\n",
    "                #print(t)\n",
    "                x=opposite[x]\n",
    "                #print(t,x)\n",
    "            if(x in val):\n",
    "                val[x]+=1\n",
    "            else:\n",
    "                val[x]=1\n",
    "        \"\"\"\n",
    "    #print(mark_neg)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "8501\n"
     ]
    }
   ],
   "source": [
    "analysed = [normal_algo(txt) for txt in df['Text']]\n",
    "negatives = set(negatives)\n",
    "qmark = []\n",
    "exmark = []\n",
    "f=0\n",
    "for txt in df['Text']:\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='?'):\n",
    "            qmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        qmark.append(0)\n",
    "    f=1\n",
    "    for lt in txt:\n",
    "        if(lt=='!'):\n",
    "            exmark.append(1)\n",
    "            f=0\n",
    "            break\n",
    "    if(f==1):\n",
    "        exmark.append(0)\n",
    "print(len(qmark))\n",
    "print(len(exmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7425\n",
      "1    1076\n",
      "dtype: int64\n",
      "0    6043\n",
      "1    2458\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(qmark).value_counts())\n",
    "print(pd.Series(exmark).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Admiration</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Analysed</th>\n",
       "      <th>qmark</th>\n",
       "      <th>exmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So much for sleeping in.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so much sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College days are loooong days.. 3 more hours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>college day be loooong day more hour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@daihard I'm headed to Kentucky this time. Nev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i m head kentucky time never be be fun gqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hella tired.. where is gilbert for the usual b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hella tire be gilbert usual basketball talk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not as dry this morning as would have liked  l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not dry morning a have like lot moisture dune ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lil_laura_loo Really? I think we have some! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>laura loo really i think have i ve take pirite...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Do512_Kristin it's a good thing they give you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kristin s good thing give xanax something i ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PB&amp;amp;J, Owl City, and boredom.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pb amp j owl city boredom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So it's Saturday again &amp;amp; what do I do..? W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s saturday again amp do i do work again course</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trying to relax and watch Nascar, Difficult 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>try relax watch nascar difficult cause child d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Joy  Sadness  Anger  \\\n",
       "0                          So much for sleeping in.     0        0      0   \n",
       "1      College days are loooong days.. 3 more hours     0        1      0   \n",
       "2  @daihard I'm headed to Kentucky this time. Nev...    0        0      0   \n",
       "3  hella tired.. where is gilbert for the usual b...    0        0      0   \n",
       "4  Not as dry this morning as would have liked  l...    0        1      0   \n",
       "5  @lil_laura_loo Really? I think we have some! I...    0        1      0   \n",
       "6  @Do512_Kristin it's a good thing they give you...    0        0      0   \n",
       "7                  PB&amp;J, Owl City, and boredom.     0        1      0   \n",
       "8  So it's Saturday again &amp; what do I do..? W...    0        1      0   \n",
       "9  trying to relax and watch Nascar, Difficult 'c...    0        1      0   \n",
       "\n",
       "   Disgust  Admiration  Surprise  Interest  Fear  \\\n",
       "0        0           0         0         0     1   \n",
       "1        0           0         0         1     0   \n",
       "2        0           0         0         1     0   \n",
       "3        0           0         0         1     0   \n",
       "4        1           0         0         0     0   \n",
       "5        0           0         0         0     1   \n",
       "6        1           0         0         0     0   \n",
       "7        1           0         0         0     0   \n",
       "8        0           0         0         0     0   \n",
       "9        1           0         0         0     0   \n",
       "\n",
       "                                            Analysed  qmark  exmark  \n",
       "0                                     so much sleep       0       0  \n",
       "1              college day be loooong day more hour       0       0  \n",
       "2        i m head kentucky time never be be fun gqz       1       1  \n",
       "3       hella tire be gilbert usual basketball talk       1       1  \n",
       "4  not dry morning a have like lot moisture dune ...      0       1  \n",
       "5  laura loo really i think have i ve take pirite...      1       1  \n",
       "6  kristin s good thing give xanax something i ba...      0       0  \n",
       "7                         pb amp j owl city boredom       0       0  \n",
       "8    s saturday again amp do i do work again course       1       0  \n",
       "9  try relax watch nascar difficult cause child d...      0       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Analysed'] = analysed\n",
    "df['qmark'] = qmark\n",
    "df['exmark'] = exmark\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n"
     ]
    }
   ],
   "source": [
    "def convtodec(x):\n",
    "    val = 128\n",
    "    ret = 0\n",
    "    for y in x:\n",
    "        if(y):\n",
    "            ret+=val\n",
    "        val=val>>1\n",
    "    return ret\n",
    "labelpowerset = []\n",
    "for row in df.iterrows():\n",
    "    tmp = row[1][1:9].tolist()\n",
    "    labelpowerset.append(convtodec(tmp))\n",
    "print(len(labelpowerset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "cntpowerset = {}\n",
    "for val in labelpowerset:\n",
    "    if(val in cntpowerset):\n",
    "        cntpowerset[val]+=1\n",
    "    else:\n",
    "        cntpowerset[val]=1\n",
    "\n",
    "cntpowersetlist = []\n",
    "for key,val in cntpowerset.items():\n",
    "    cntpowersetlist.append((val,key))\n",
    "cntpowersetlist.sort(reverse=True)\n",
    "print(len(cntpowersetlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {}\n",
    "revlabel = {}\n",
    "cnt = 0\n",
    "for val in cntpowersetlist:\n",
    "    label[val[1]] = cnt\n",
    "    revlabel[cnt] = val[1]\n",
    "    cnt+=1\n",
    "powset = [label[x] for x in labelpowerset]\n",
    "df['powerset'] = powset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1870\n",
       "1      998\n",
       "2      772\n",
       "3      711\n",
       "4      348\n",
       "      ... \n",
       "85       1\n",
       "93       1\n",
       "86       1\n",
       "94       1\n",
       "91       1\n",
       "Name: powerset, Length: 95, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['powerset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7395\n"
     ]
    }
   ],
   "source": [
    "df_tmp = df[df['powerset']<=20]\n",
    "print(len(df_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senticnet(word,em):\n",
    "    em = '#'+em.lower()\n",
    "    if(senticnet[word][4]==em or senticnet[word][5]==em):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tmp[['Analysed','qmark','exmark']]\n",
    "Y = df_tmp['powerset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9848\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tmp['Analysed'])\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(score_list,predict_score_list):\n",
    "    filter_corr = []\n",
    "    exmatch = 0\n",
    "    atleast1 = 0\n",
    "    md1 = 0\n",
    "    one_f = 0\n",
    "    more_f = 0\n",
    "    zero_f = 0\n",
    "    sm = 0\n",
    "    sdensity = 0\n",
    "    hammval = 0\n",
    "    test_len = len(predict_score_list[0])\n",
    "    for j in range(test_len):\n",
    "        cnt=0\n",
    "        for i in range(8):\n",
    "            hammval+=(score_list[i][j] ^ int(predict_score_list[i][j]))\n",
    "            if(score_list[i][j]==1):\n",
    "                cnt+=1\n",
    "                sm+=1\n",
    "        sdensity+=cnt/8\n",
    "        if(cnt==0):\n",
    "            zero_f+=1\n",
    "        if(cnt==1):\n",
    "            one_f+=1\n",
    "        if(cnt>1):\n",
    "            more_f+=1\n",
    "        for i in range(8):\n",
    "            mf = True\n",
    "            if(int(predict_score_list[i][j])!=score_list[i][j]):\n",
    "                mf=False\n",
    "                break\n",
    "        if(mf==True):\n",
    "            exmatch+=1\n",
    "            filter_corr.append(j)\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                atleast1+=1\n",
    "                break\n",
    "        mf = False\n",
    "        for i in range(8):\n",
    "            if(int(predict_score_list[i][j])==score_list[i][j] and score_list[i][j]==1):\n",
    "                if(mf==True):\n",
    "                    md1+=1\n",
    "                    filter_corr.append(j)\n",
    "                    break\n",
    "                mf=True\n",
    "    #print(\"Label Cardinality: \"+ str(sm/test_len))\n",
    "    #print(\"Label Density: \"+ str(sdensity/test_len))\n",
    "    print(\"Hamming Loss: \"+str(hammval/(test_len*8)))\n",
    "    hamlos = hammval/(test_len*8)\n",
    "    print(\"Exact Prediction: \"+str(exmatch/test_len))\n",
    "    sub_accu = exmatch/test_len\n",
    "    #print(\"At least one label predicted: \"+str(atleast1/(test_len-zero_f)))\n",
    "    #print(\"More than one label predicted: \"+str(md1/more_f))\n",
    "    tp_sum = 0\n",
    "    fp_sum = 0\n",
    "    fn_sum = 0\n",
    "    macro_preci = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for i in range(len(score_list)):\n",
    "        tmp = confusion_matrix(score_list[i],predict_score_list[i])\n",
    "        tp_sum+=tmp[0][0]\n",
    "        fp_sum+=tmp[0][1]\n",
    "        fn_sum+=tmp[1][0]\n",
    "        macro_preci_tmp=tmp[0][0]/(tmp[0][0]+tmp[0][1])\n",
    "        macro_recall_tmp=tmp[0][0]/(tmp[0][0]+tmp[1][0])\n",
    "        macro_f1 += ((2*macro_preci_tmp*macro_recall_tmp)/(macro_preci_tmp+macro_recall_tmp))\n",
    "        macro_preci+=macro_preci_tmp\n",
    "        macro_recall+=macro_recall_tmp\n",
    "        #print(macro_f1)\n",
    "    micro_preci = tp_sum/(tp_sum+fp_sum)\n",
    "    micro_recall = tp_sum/(tp_sum+fn_sum)\n",
    "    micro_f1 = (2*micro_preci*micro_recall)/(micro_preci+micro_recall)\n",
    "    macro_preci/=8\n",
    "    macro_recall/=8\n",
    "    macro_f1/=8\n",
    "    #print(micro_preci,micro_recall,micro_f1)\n",
    "    #print(macro_preci,macro_recall,macro_f1)\n",
    "    print(\"Macro F-Score: \"+str(macro_f1))\n",
    "    print(\"Micro F-Score: \"+str(micro_f1))\n",
    "    col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "    tmp = 0\n",
    "    for i in range(len(score_list)):\n",
    "        score = accuracy_score(score_list[i],predict_score_list[i]) \n",
    "        #print(col_names[i]+\" accuracy: \"+str(score))\n",
    "        tmp += score\n",
    "    print(\"Average Accuracy: \" + str(tmp/8))\n",
    "    avg_accu = tmp/8\n",
    "    return (hamlos,sub_accu,macro_f1,micro_f1,avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2bin(xx):\n",
    "    ret = []\n",
    "    for i in range(8):\n",
    "        if(xx & (1<<i)):\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    #print(xx)\n",
    "    ret.reverse()\n",
    "    #print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.1445945945945946\n",
      "Exact Prediction: 0.33918918918918917\n",
      "Macro F-Score: 0.8899636146475027\n",
      "Micro F-Score: 0.9151467089611418\n",
      "Average Accuracy: 0.8554054054054053\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(8281, 9850) (740, 9850)\n",
      "(8281,) (740,)\n",
      "Hamming Loss: 0.14831081081081082\n",
      "Exact Prediction: 0.33513513513513515\n",
      "Macro F-Score: 0.8877695479528243\n",
      "Micro F-Score: 0.9131037212984957\n",
      "Average Accuracy: 0.8516891891891891\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.15320945945945946\n",
      "Exact Prediction: 0.3027027027027027\n",
      "Macro F-Score: 0.8869642557721665\n",
      "Micro F-Score: 0.9095261845386533\n",
      "Average Accuracy: 0.8467905405405406\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(8317, 9850) (740, 9850)\n",
      "(8317,) (740,)\n",
      "Hamming Loss: 0.1516891891891892\n",
      "Exact Prediction: 0.31216216216216214\n",
      "Macro F-Score: 0.8853746275512212\n",
      "Micro F-Score: 0.9107178365480215\n",
      "Average Accuracy: 0.8483108108108107\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(8294, 9850) (740, 9850)\n",
      "(8294,) (740,)\n",
      "Hamming Loss: 0.15033783783783783\n",
      "Exact Prediction: 0.31216216216216214\n",
      "Macro F-Score: 0.8867380899160698\n",
      "Micro F-Score: 0.9117588736862978\n",
      "Average Accuracy: 0.8496621621621621\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(8327, 9850) (739, 9850)\n",
      "(8327,) (739,)\n",
      "Hamming Loss: 0.15290933694181327\n",
      "Exact Prediction: 0.3004059539918809\n",
      "Macro F-Score: 0.8867789774456383\n",
      "Micro F-Score: 0.9100855380942908\n",
      "Average Accuracy: 0.8470906630581867\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(8284, 9850) (739, 9850)\n",
      "(8284,) (739,)\n",
      "Hamming Loss: 0.14783491204330176\n",
      "Exact Prediction: 0.3044654939106901\n",
      "Macro F-Score: 0.8892003935453743\n",
      "Micro F-Score: 0.9126873126873126\n",
      "Average Accuracy: 0.8521650879566982\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(8271, 9850) (739, 9850)\n",
      "(8271,) (739,)\n",
      "Hamming Loss: 0.15460081190798375\n",
      "Exact Prediction: 0.3125845737483085\n",
      "Macro F-Score: 0.8849323593325432\n",
      "Micro F-Score: 0.9092533756949959\n",
      "Average Accuracy: 0.8453991880920162\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(8293, 9850) (739, 9850)\n",
      "(8293,) (739,)\n",
      "Hamming Loss: 0.14665087956698242\n",
      "Exact Prediction: 0.3301759133964817\n",
      "Macro F-Score: 0.8890644719536555\n",
      "Micro F-Score: 0.9136368164159777\n",
      "Average Accuracy: 0.8533491204330176\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(8332, 9850) (739, 9850)\n",
      "(8332,) (739,)\n",
      "Hamming Loss: 0.15003382949932342\n",
      "Exact Prediction: 0.3166441136671177\n",
      "Macro F-Score: 0.8856981049891347\n",
      "Micro F-Score: 0.9117676315527703\n",
      "Average Accuracy: 0.8499661705006765\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.15001716618512967\n",
      "Average Subset Accuracy: 0.316562740006583\n",
      "Average Macro F-score: 0.887248444310613\n",
      "Average Micro F-score: 0.9117683999477956\n",
      "Average of Average Accuracy: 0.8499828338148703\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = RandomForestClassifier(n_estimators=300)\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(word,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.144595</td>\n",
       "      <td>0.339189</td>\n",
       "      <td>0.889964</td>\n",
       "      <td>0.915147</td>\n",
       "      <td>0.855405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.148311</td>\n",
       "      <td>0.335135</td>\n",
       "      <td>0.887770</td>\n",
       "      <td>0.913104</td>\n",
       "      <td>0.851689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.153209</td>\n",
       "      <td>0.302703</td>\n",
       "      <td>0.886964</td>\n",
       "      <td>0.909526</td>\n",
       "      <td>0.846791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.151689</td>\n",
       "      <td>0.312162</td>\n",
       "      <td>0.885375</td>\n",
       "      <td>0.910718</td>\n",
       "      <td>0.848311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.150338</td>\n",
       "      <td>0.312162</td>\n",
       "      <td>0.886738</td>\n",
       "      <td>0.911759</td>\n",
       "      <td>0.849662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.152909</td>\n",
       "      <td>0.300406</td>\n",
       "      <td>0.886779</td>\n",
       "      <td>0.910086</td>\n",
       "      <td>0.847091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.147835</td>\n",
       "      <td>0.304465</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.912687</td>\n",
       "      <td>0.852165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.154601</td>\n",
       "      <td>0.312585</td>\n",
       "      <td>0.884932</td>\n",
       "      <td>0.909253</td>\n",
       "      <td>0.845399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.330176</td>\n",
       "      <td>0.889064</td>\n",
       "      <td>0.913637</td>\n",
       "      <td>0.853349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.150034</td>\n",
       "      <td>0.316644</td>\n",
       "      <td>0.885698</td>\n",
       "      <td>0.911768</td>\n",
       "      <td>0.849966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.150017</td>\n",
       "      <td>0.316563</td>\n",
       "      <td>0.887248</td>\n",
       "      <td>0.911768</td>\n",
       "      <td>0.849983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.144595         0.339189       0.889964       0.915147   \n",
       "1         2      0.148311         0.335135       0.887770       0.913104   \n",
       "2         3      0.153209         0.302703       0.886964       0.909526   \n",
       "3         4      0.151689         0.312162       0.885375       0.910718   \n",
       "4         5      0.150338         0.312162       0.886738       0.911759   \n",
       "5         6      0.152909         0.300406       0.886779       0.910086   \n",
       "6         7      0.147835         0.304465       0.889200       0.912687   \n",
       "7         8      0.154601         0.312585       0.884932       0.909253   \n",
       "8         9      0.146651         0.330176       0.889064       0.913637   \n",
       "9        10      0.150034         0.316644       0.885698       0.911768   \n",
       "10  average      0.150017         0.316563       0.887248       0.911768   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.855405  \n",
       "1           0.851689  \n",
       "2           0.846791  \n",
       "3           0.848311  \n",
       "4           0.849662  \n",
       "5           0.847091  \n",
       "6           0.852165  \n",
       "7           0.845399  \n",
       "8           0.853349  \n",
       "9           0.849966  \n",
       "10          0.849983  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold validation: 1\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.1464527027027027\n",
      "Exact Prediction: 0.327027027027027\n",
      "Macro F-Score: 0.8798959172213772\n",
      "Micro F-Score: 0.9139112302651176\n",
      "Average Accuracy: 0.8535472972972973\n",
      "\n",
      "\n",
      "k_fold validation: 2\n",
      "(8281, 9850) (740, 9850)\n",
      "(8281,) (740,)\n",
      "Hamming Loss: 0.15371621621621623\n",
      "Exact Prediction: 0.3324324324324324\n",
      "Macro F-Score: 0.877362330176236\n",
      "Micro F-Score: 0.9097222222222222\n",
      "Average Accuracy: 0.8462837837837839\n",
      "\n",
      "\n",
      "k_fold validation: 3\n",
      "(8291, 9850) (740, 9850)\n",
      "(8291,) (740,)\n",
      "Hamming Loss: 0.15675675675675677\n",
      "Exact Prediction: 0.29324324324324325\n",
      "Macro F-Score: 0.8772920302613761\n",
      "Micro F-Score: 0.9074591144794576\n",
      "Average Accuracy: 0.8432432432432432\n",
      "\n",
      "\n",
      "k_fold validation: 4\n",
      "(8317, 9850) (740, 9850)\n",
      "(8317,) (740,)\n",
      "Hamming Loss: 0.14560810810810812\n",
      "Exact Prediction: 0.3310810810810811\n",
      "Macro F-Score: 0.8810545159382425\n",
      "Micro F-Score: 0.9141776184786938\n",
      "Average Accuracy: 0.8543918918918919\n",
      "\n",
      "\n",
      "k_fold validation: 5\n",
      "(8294, 9850) (740, 9850)\n",
      "(8294,) (740,)\n",
      "Hamming Loss: 0.15287162162162163\n",
      "Exact Prediction: 0.31351351351351353\n",
      "Macro F-Score: 0.878563603215996\n",
      "Micro F-Score: 0.9102449667757612\n",
      "Average Accuracy: 0.8471283783783783\n",
      "\n",
      "\n",
      "k_fold validation: 6\n",
      "(8327, 9850) (739, 9850)\n",
      "(8327,) (739,)\n",
      "Hamming Loss: 0.15493910690121787\n",
      "Exact Prediction: 0.31799729364005414\n",
      "Macro F-Score: 0.8769340200400996\n",
      "Micro F-Score: 0.9087649402390439\n",
      "Average Accuracy: 0.8450608930987822\n",
      "\n",
      "\n",
      "k_fold validation: 7\n",
      "(8284, 9850) (739, 9850)\n",
      "(8284,) (739,)\n",
      "Hamming Loss: 0.1564614343707713\n",
      "Exact Prediction: 0.2814614343707713\n",
      "Macro F-Score: 0.8758323087060024\n",
      "Micro F-Score: 0.9076569831286811\n",
      "Average Accuracy: 0.8435385656292286\n",
      "\n",
      "\n",
      "k_fold validation: 8\n",
      "(8271, 9850) (739, 9850)\n",
      "(8271,) (739,)\n",
      "Hamming Loss: 0.1507104194857916\n",
      "Exact Prediction: 0.34370771312584575\n",
      "Macro F-Score: 0.8806145776457668\n",
      "Micro F-Score: 0.9115105770185719\n",
      "Average Accuracy: 0.8492895805142084\n",
      "\n",
      "\n",
      "k_fold validation: 9\n",
      "(8293, 9850) (739, 9850)\n",
      "(8293,) (739,)\n",
      "Hamming Loss: 0.14817320703653586\n",
      "Exact Prediction: 0.3301759133964817\n",
      "Macro F-Score: 0.8812421999601198\n",
      "Micro F-Score: 0.9127142287764051\n",
      "Average Accuracy: 0.8518267929634642\n",
      "\n",
      "\n",
      "k_fold validation: 10\n",
      "(8332, 9850) (739, 9850)\n",
      "(8332,) (739,)\n",
      "Hamming Loss: 0.1510487144790257\n",
      "Exact Prediction: 0.3342354533152909\n",
      "Macro F-Score: 0.8764552669772318\n",
      "Micro F-Score: 0.9110114598903838\n",
      "Average Accuracy: 0.8489512855209743\n",
      "\n",
      "\n",
      "Final Result: \n",
      "Average Hamming Loss: 0.15167382876787477\n",
      "Average Subset Accuracy: 0.320487510514574\n",
      "Average Macro F-score: 0.8785246770142449\n",
      "Average Micro F-score: 0.9107173341274338\n",
      "Average of Average Accuracy: 0.8483261712321252\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Joy','Sadness','Anger','Disgust','Admiration','Surprise','Interest','Fear']\n",
    "hamm_score = []\n",
    "subset_accu = []\n",
    "macro_f1 = []\n",
    "micro_f1 = []\n",
    "avg_accu = []\n",
    "cnt = 1\n",
    "for train_index,test_index in kf.split(X):\n",
    "    clf = SVC()\n",
    "    x_train,x_test = X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test = Y.iloc[train_index].tolist(),Y.iloc[test_index].tolist()\n",
    "    print(\"k_fold validation: \" + str(cnt))\n",
    "    cnt+=1\n",
    "    x_train_analysed = x_train['Analysed'].tolist()\n",
    "    x_train_qmark = x_train['qmark'].tolist()\n",
    "    x_train_exmark = x_train['exmark'].tolist()\n",
    "    x_test_analysed = x_test['Analysed'].tolist()\n",
    "    x_test_qmark = x_test['qmark'].tolist()\n",
    "    x_test_exmark = x_test['exmark'].tolist()\n",
    "    pre = {}\n",
    "    for sen in x_train_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            pre[t]=1\n",
    "    for sen in x_test_analysed:\n",
    "        tok = word_tokenize(sen)\n",
    "        for t in tok:\n",
    "            if(t in pre):\n",
    "                continue\n",
    "            else:\n",
    "                if(t in senticnet):\n",
    "                    x_train_analysed.append(t)\n",
    "                    x_train_qmark.append(0)\n",
    "                    x_train_exmark.append(0)\n",
    "                    tmp_list = []\n",
    "                    for cl in col_names:\n",
    "                        tmp_list.append(get_senticnet(t,cl))\n",
    "                    y_train.append(label[convtodec(tmp_list)])\n",
    "    for word in negatives:\n",
    "        if(word in senticnet):\n",
    "            x_train_analysed.append(\"not \"+word)\n",
    "            x_train_qmark.append(0)\n",
    "            x_train_exmark.append(0)\n",
    "            tmp_list = []\n",
    "            for cl in col_names:\n",
    "                tmp_list.append(get_senticnet(t,cl))\n",
    "            tmp_list2 = []\n",
    "            for i in range(8):\n",
    "                tmp_list2.append(0)\n",
    "            for i in range(8):\n",
    "                if(tmp_list[i]==1):\n",
    "                    tmp_list2[opposite[i]] = 1\n",
    "            y_train.append(label[convtodec(tmp_list2)])\n",
    "    x_train_analysed_vec = vectorizer.transform(x_train_analysed)\n",
    "    x_test_analysed_vec = vectorizer.transform(x_test_analysed)\n",
    "    tmp = sparse.hstack((x_train_analysed_vec,np.array(x_train_qmark)[:,None]))\n",
    "    x_train = sparse.hstack((tmp,np.array(x_train_exmark)[:,None]))\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    tmp = sparse.hstack((x_test_analysed_vec,np.array(x_test_qmark)[:,None]))\n",
    "    x_test = sparse.hstack((tmp,np.array(x_test_exmark)[:,None]))\n",
    "    \n",
    "    print(x_train.shape,x_test.shape)\n",
    "    print(y_train.shape,y_test.shape)\n",
    "    \n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score_list = []\n",
    "    predict_score_list = []\n",
    "    for i in range(len(y_test)):\n",
    "        score_list.append(convert2bin(revlabel[y_test[i]]))\n",
    "        predict_score_list.append(convert2bin(revlabel[y_pred[i]]))\n",
    "    np_score_list = np.array(score_list)\n",
    "    transpose = np_score_list.T\n",
    "    score_list = transpose.tolist()\n",
    "\n",
    "    np_predict_score_list = np.array(predict_score_list)\n",
    "    transpose = np_predict_score_list.T\n",
    "    predict_score_list = transpose.tolist()\n",
    "    \n",
    "    ret = evaluation(score_list,predict_score_list)\n",
    "    hamm_score.append(ret[0])\n",
    "    subset_accu.append(ret[1])\n",
    "    macro_f1.append(ret[2])\n",
    "    micro_f1.append(ret[3])\n",
    "    avg_accu.append(ret[4])\n",
    "    print('\\n')\n",
    "print('Final Result: ')\n",
    "print('Average Hamming Loss: '+str(sum(hamm_score)/len(hamm_score)))\n",
    "print('Average Subset Accuracy: '+str(sum(subset_accu)/len(subset_accu)))\n",
    "print('Average Macro F-score: '+str(sum(macro_f1)/len(macro_f1)))\n",
    "print('Average Micro F-score: '+str(sum(micro_f1)/len(micro_f1)))\n",
    "print('Average of Average Accuracy: '+str(sum(avg_accu)/len(avg_accu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i+1for i in range(10)]\n",
    "labels.append('average')\n",
    "hamm_score.append(sum(hamm_score)/len(hamm_score))\n",
    "subset_accu.append(sum(subset_accu)/len(subset_accu))\n",
    "macro_f1.append(sum(macro_f1)/len(macro_f1))\n",
    "micro_f1.append(sum(micro_f1)/len(micro_f1))\n",
    "avg_accu.append(sum(avg_accu)/len(avg_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-fold</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Subset accuracy</th>\n",
       "      <th>Macro F-score</th>\n",
       "      <th>Micro F-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.146453</td>\n",
       "      <td>0.327027</td>\n",
       "      <td>0.879896</td>\n",
       "      <td>0.913911</td>\n",
       "      <td>0.853547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.153716</td>\n",
       "      <td>0.332432</td>\n",
       "      <td>0.877362</td>\n",
       "      <td>0.909722</td>\n",
       "      <td>0.846284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.156757</td>\n",
       "      <td>0.293243</td>\n",
       "      <td>0.877292</td>\n",
       "      <td>0.907459</td>\n",
       "      <td>0.843243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.145608</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.881055</td>\n",
       "      <td>0.914178</td>\n",
       "      <td>0.854392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.152872</td>\n",
       "      <td>0.313514</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.847128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.317997</td>\n",
       "      <td>0.876934</td>\n",
       "      <td>0.908765</td>\n",
       "      <td>0.845061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.156461</td>\n",
       "      <td>0.281461</td>\n",
       "      <td>0.875832</td>\n",
       "      <td>0.907657</td>\n",
       "      <td>0.843539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.150710</td>\n",
       "      <td>0.343708</td>\n",
       "      <td>0.880615</td>\n",
       "      <td>0.911511</td>\n",
       "      <td>0.849290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.148173</td>\n",
       "      <td>0.330176</td>\n",
       "      <td>0.881242</td>\n",
       "      <td>0.912714</td>\n",
       "      <td>0.851827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.151049</td>\n",
       "      <td>0.334235</td>\n",
       "      <td>0.876455</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.848951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average</td>\n",
       "      <td>0.151674</td>\n",
       "      <td>0.320488</td>\n",
       "      <td>0.878525</td>\n",
       "      <td>0.910717</td>\n",
       "      <td>0.848326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k-fold  Hamming loss  Subset accuracy  Macro F-score  Micro F-score  \\\n",
       "0         1      0.146453         0.327027       0.879896       0.913911   \n",
       "1         2      0.153716         0.332432       0.877362       0.909722   \n",
       "2         3      0.156757         0.293243       0.877292       0.907459   \n",
       "3         4      0.145608         0.331081       0.881055       0.914178   \n",
       "4         5      0.152872         0.313514       0.878564       0.910245   \n",
       "5         6      0.154939         0.317997       0.876934       0.908765   \n",
       "6         7      0.156461         0.281461       0.875832       0.907657   \n",
       "7         8      0.150710         0.343708       0.880615       0.911511   \n",
       "8         9      0.148173         0.330176       0.881242       0.912714   \n",
       "9        10      0.151049         0.334235       0.876455       0.911011   \n",
       "10  average      0.151674         0.320488       0.878525       0.910717   \n",
       "\n",
       "    Average Accuracy  \n",
       "0           0.853547  \n",
       "1           0.846284  \n",
       "2           0.843243  \n",
       "3           0.854392  \n",
       "4           0.847128  \n",
       "5           0.845061  \n",
       "6           0.843539  \n",
       "7           0.849290  \n",
       "8           0.851827  \n",
       "9           0.848951  \n",
       "10          0.848326  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svc = pd.DataFrame(list(zip(labels,hamm_score,subset_accu,macro_f1,micro_f1,avg_accu)),\n",
    "              columns = ['k-fold','Hamming loss','Subset accuracy','Macro F-score','Micro F-score','Average Accuracy'])\n",
    "df_svc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
